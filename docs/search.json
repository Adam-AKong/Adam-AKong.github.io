[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adam Kong",
    "section": "",
    "text": "Hello! I am a Graduate from California Polytechnic State University in San Luis Obispo. I graduated with a degree in Statistics with a Computer Science Minor. My interest are in the Actuary and Insurance Fields. I am currently working on studying for the P Exam and working on personal projects. In my free time I love doing Sports Analysis and exercising.\n\n\n\nCalifornia Polytechnic State University, California | San Luis Obispo, CA\nB.S. in Statistics | Computer Science Minor | 2021 - 2025\n\n \n  \n   \n  \n     LinkedIn\n  \n  \n     GitHub"
  },
  {
    "objectID": "posts/STAT414Project/index.html",
    "href": "posts/STAT414Project/index.html",
    "title": "US Honey Production Project",
    "section": "",
    "text": "US Honey Production Analysis\nHoney production in the United States is influenced by multiple factors, including environmental conditions, colony numbers, and market prices. This study examines key predictors of honey production value using data from 1995 to 2021, focusing on whether these factors vary by state and over time. The data, sourced from the US Honey Production dataset on Kaggle, is structured hierarchically with two levels. The first level consists of yearly observations, including variables such as Colonies Number, Yield Per Colony, Production, Stocks, and Average Price, while the second level accounts for state-level factors like land mass. Additional variables that were not included in the original data set were temperature, grassland size, and agricultural field size which were considered but excluded due to redundancy and insignificance.\nTo analyze these factors, a mixed-effects model was applied. The model included fixed effects for colonies number, stocks, year, and their interaction, with an intercept of 3,075,000. The number of colonies had a significant positive effect (85.45), while stocks showed a negative association (-1.424). The interaction between stocks and year was also found to be statistically significant (0.1052). The results revealed substantial variability in honey production value across states, with state-level variance estimated at 7.68e+12. Additionally, the impact of colonies number varied by state, with a slope variance of 3,664. Despite these identified factors, a significant portion of the variability remained unexplained within states (5.59e+12). Overall, the model accounted for 57% of the total variation in honey production value.\nThe findings suggest that the number of colonies and stock levels are the strongest predictors of honey production value, with their effects varying across states and over time. This highlights the importance of localized factors in honey production. Future studies should explore additional environmental and economic variables to enhance predictive accuracy and provide deeper insights into the dynamics of honey production in the United States.\n\n\n\n\n\nEmbedded Slides"
  },
  {
    "objectID": "posts/STAT415Project/index.html",
    "href": "posts/STAT415Project/index.html",
    "title": "2023 Basketball Bayesian Analysis",
    "section": "",
    "text": "Our research question investigates how usage rate (USG) and minutes played per game (MP) affect an NBA player’s annual salary for the 2022-23 season. Usage rate is the percentage of a team’s possessions a player ends while on the court. It’s calculated by adding up a player’s field goal attempts, turnovers, and trips to the free throw line, and then dividing that by the team’s total of those plays when the player is on the court. Minutes played per game describes how many in-game minutes (there are 48 in-game minutes in a regular NBA game) a player averages per game.\nThis data set contains numerous variables for players in the NBA. We are using USG and MP (as well as their interaction) as explanatory variables and log(Salary) as the response variable.\nThe data set includes numerous variables for NBA players during the 2022-23 season. We decided to focus on the usage rate and minutes played per game statistics and how that affects a player’s salary. The data is publicly available because players’ salaries are reported by agents and reporters and in-game stats are often released by NBA stat keepers. Kaggle (where we got the data from) compiled the data set from numerous websites and databases.\n\nPre-Bayesian Analysis\nAs seen below these explanatory variables are not very normal. However, we went into our analysis by transforming the response variable of salaries since salaries are much more likely to be skewed to the right without previous insepction of the dataset, since a few select players will have extreme salaries.\n\npar(mfrow = c(1, 2))\nplot(nba_salary$USG., nba_salary$Salary)\nplot(nba_salary$MP, nba_salary$Salary)\n\n\n\n\nPossible transformations and inspection of data before analysis.\n\npar(mfrow = c(2, 2))\nhist(nba_salary$Salary)\nhist(sqrt(nba_salary$Salary))\nhist(log(nba_salary$Salary))\nhist(nba_salary$Salary/1000000)\n\n\n\n\n\n\nProposed Bayesian Model\n\\[{\\log(Y)} = {\\beta_0} + {\\beta_1}(USG) + {\\beta_2}(MP) + {\\beta_3}(USG*MP) + {\\varepsilon}\\]\n\n\nChoice of Likelihood\nWe used a Normal likelihood to calculate the probability of observing a certain log(Salary) given certain parameters for the betas.\n\n\nAssumptions\nLinearity: The relationship between log(Salary) and the explanatory variables is linear Homoscedasticity: variance of the residuals is constant Independence of errors: Residuals are independent of each other Normally distributed: Residuals are approximately normally distributed Independence of independent variables: included interaction effect in model to see how variables interact with each other.\n\n\nBayesian Model Analysis\n\ntest_01 <- lm(log(Salary) ~ MP + USG. + MP*USG., data = nba_salary)\nsummary(test_01)\n\n\nCall:\nlm(formula = log(Salary) ~ MP + USG. + MP * USG., data = nba_salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8611 -0.5311  0.2033  0.7138  2.7588 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.1414029  0.3791128  37.301  < 2e-16 ***\nMP           0.0272010  0.0179315   1.517   0.1300    \nUSG.        -0.0485190  0.0197029  -2.463   0.0142 *  \nMP:USG.      0.0033111  0.0008146   4.065 5.65e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.191 on 463 degrees of freedom\nMultiple R-squared:  0.4103,    Adjusted R-squared:  0.4064 \nF-statistic: 107.4 on 3 and 463 DF,  p-value: < 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(test_01)\n\n\n\n\nWe used BRMS to choose a prior distribution for the data. The model chose a student_t(3, 15.1, 2.5) distribution for the intercept and a student_t(3, 0, 2.5) distribution for sigma. The MP and USG variables have a flat prior, which is an improper distribution, but one that makes no assumption of the distribution.\n\n\nBRMS Chosen Prior\n\nget_prior(data = nba_salary,\n          family = gaussian(),\n          log(Salary) ~ MP + USG. + MP*USG.)\n\n                   prior     class    coef group resp dpar nlpar lb ub\n                  (flat)         b                                    \n                  (flat)         b      MP                            \n                  (flat)         b MP:USG.                            \n                  (flat)         b    USG.                            \n student_t(3, 15.1, 2.5) Intercept                                    \n    student_t(3, 0, 2.5)     sigma                                0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n\n\n\n\nBRMS (First Model)\n\nfit <- brm(data = nba_salary,\n           family = gaussian(),\n           log(Salary) ~ MP + USG. + MP*USG.,\n           sample_prior = TRUE,\n           iter = 3500,\n           warmup = 1000,\n           chains = 4,\n           refresh = 0)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nBRMS Posterior Output Analysis\nThe posterior distribution does not change very much if the prior changes. This is because we have a large sample size for our data, which means that the data is going to hold the most weight in the posterior distribution.\n\ncolor_scheme_set(\"viridis\")\n\nplot(fit)\n\n\n\n\n\nsummary(fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(Salary) ~ MP + USG. + MP * USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    14.14      0.38    13.41    14.89 1.00     4248     4638\nMP            0.03      0.02    -0.01     0.06 1.00     4643     5399\nUSG.         -0.05      0.02    -0.09    -0.01 1.00     4372     5098\nMP:USG.       0.00      0.00     0.00     0.00 1.00     4436     4666\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.19      0.04     1.12     1.27 1.00     6379     5220\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote, that we did run an interaction between minutes played and player usage. We decided to keep that in when simulated because before simulation we found that the interaction was significant, but now it was not in the posterior. We did not find this out until after the simulation was ran, but we kept because it’s notable that minutes played and usage, while correlated, it’s interaction was not significant to the log salary.\n\ncolor_scheme_set(\"purple\")\n\npp_check(fit, ndraw = 100)\n\n\n\n\nSimulated distributions follow a similar distribution with the actual curve. The actual curve has some strange curves due to the salary.\n\npairs(fit)\n\n\n\n\nFrom this plot we can see that minutes played has a positive correlation with the usage rate. This is expected as generally players with a high minutes played will have a larger usage rate.\n\nmcmc_dens_overlay(fit, pars = vars(b_Intercept, b_MP, b_USG., sigma))\n\n\n\n\nLooking good so far! The chains are very close to each other here.\n\nneff_ratio(fit)[c(\"b_Intercept\", \"b_MP\", \"b_USG.\", \"sigma\")]\n\nb_Intercept        b_MP      b_USG.       sigma \n  0.4247532   0.4642651   0.4372386   0.5219679 \n\n\n\nneff_ratio(fit)[c(\"b_Intercept\", \"b_MP\", \"b_USG.\", \"sigma\")] |> \n  mcmc_neff() +\n  yaxis_text(hjust = 0) \n\n\n\n\nLooks good.\n\nmcmc_intervals(fit,\n               pars = c(\"b_MP\"),\n               prob = 0.5,\n               prob_outer = 0.98)\n\n\n\n\nMP is a positive coefficent, just like before the simulation.\n\ncolor_scheme_set(\"teal\")\n\nmcmc_areas(fit,\n           pars = c(\"b_MP\"),\n           prob = 0.5,\n           point_est = \"median\")\n\n\n\nmcmc_areas(fit,\n           pars = c(\"b_MP\"),\n           prob = 0.8,\n           point_est = \"median\")\n\n\n\nmcmc_areas(fit,\n           pars = c(\"b_MP\"),\n           prob = 0.98,\n           point_est = \"median\")\n\n\n\n\nPossibly may have some negative simulated values, but more positive.\n\nmcmc_intervals(fit,\n               pars = c(\"b_USG.\"),\n               prob = 0.5,\n               prob_outer = 0.98)\n\n\n\n\nSurprisingly the coefficient for Usage is negative. Not very strong as usage has a range from 0 to 100, and most players have below 50 so not as big of an effect.\n\ncolor_scheme_set(\"teal\")\n\nmcmc_areas(fit,\n           pars = c(\"b_USG.\"),\n           prob = 0.5,\n           point_est = \"median\")\n\n\n\nmcmc_areas(fit,\n           pars = c(\"b_USG.\"),\n           prob = 0.8,\n           point_est = \"median\")\n\n\n\nmcmc_areas(fit,\n           pars = c(\"b_USG.\"),\n           prob = 0.98,\n           point_est = \"median\")\n\n\n\n\n\nposterior = fit |>\n  spread_draws(b_Intercept, sigma)\n\nposterior |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    sigma \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.93006 \n    1.170048 \n  \n  \n    1 \n    2 \n    2 \n    14.02398 \n    1.143928 \n  \n  \n    1 \n    3 \n    3 \n    14.19821 \n    1.210008 \n  \n  \n    1 \n    4 \n    4 \n    13.87291 \n    1.177849 \n  \n  \n    1 \n    5 \n    5 \n    14.62475 \n    1.207042 \n  \n  \n    1 \n    6 \n    6 \n    14.61113 \n    1.208182 \n  \n  \n    1 \n    7 \n    7 \n    13.71179 \n    1.202079 \n  \n  \n    1 \n    8 \n    8 \n    13.28116 \n    1.162212 \n  \n  \n    1 \n    9 \n    9 \n    14.10877 \n    1.172498 \n  \n  \n    1 \n    10 \n    10 \n    14.37647 \n    1.240193 \n  \n\n\n\n\n\n\nposterior_mean = fit |>\n  spread_draws(b_Intercept) |>\n  rename(Intercept_y = b_Intercept)\n\nposterior_mean |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    Intercept_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.93006 \n  \n  \n    1 \n    2 \n    2 \n    14.02398 \n  \n  \n    1 \n    3 \n    3 \n    14.19821 \n  \n  \n    1 \n    4 \n    4 \n    13.87291 \n  \n  \n    1 \n    5 \n    5 \n    14.62475 \n  \n  \n    1 \n    6 \n    6 \n    14.61113 \n  \n  \n    1 \n    7 \n    7 \n    13.71179 \n  \n  \n    1 \n    8 \n    8 \n    13.28116 \n  \n  \n    1 \n    9 \n    9 \n    14.10877 \n  \n  \n    1 \n    10 \n    10 \n    14.37647 \n  \n\n\n\n\n\n\nposterior_sigma = fit |>\n  spread_draws(sigma) |>\n  rename(sigma_y = sigma)\n\nposterior_sigma |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    sigma_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    1.170048 \n  \n  \n    1 \n    2 \n    2 \n    1.143928 \n  \n  \n    1 \n    3 \n    3 \n    1.210008 \n  \n  \n    1 \n    4 \n    4 \n    1.177849 \n  \n  \n    1 \n    5 \n    5 \n    1.207042 \n  \n  \n    1 \n    6 \n    6 \n    1.208182 \n  \n  \n    1 \n    7 \n    7 \n    1.202079 \n  \n  \n    1 \n    8 \n    8 \n    1.162212 \n  \n  \n    1 \n    9 \n    9 \n    1.172498 \n  \n  \n    1 \n    10 \n    10 \n    1.240193 \n  \n\n\n\n\n\n\n\nPosterior Population Analysis\n\nquantile(posterior$b_Intercept, c(0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99))\n\n      1%      10%      25%      50%      75%      90%      99% \n13.26899 13.66008 13.87919 14.13453 14.39805 14.62570 15.02639 \n\n\nThere is a 50% chance that the population mean salary is between \\(e^{13.87}\\) and \\(e^{14.38}\\) dollars. There is an 80% chance that the population mean salary is between \\(e^{13.63}\\) and \\(e^{14.64}\\) dollars. There is a 98% chance that the population mean salary is between \\(e^{13.23}\\) and \\(e^{15.04}\\) dollars.\n\\(e^{13.23}\\) is roughly around 556,821.5 dollars and \\(e^{15.04}\\) is roughly around 3,402,429 dollars\n\nquantile(posterior$sigma, c(0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99))\n\n      1%      10%      25%      50%      75%      90%      99% \n1.106558 1.143666 1.166100 1.192325 1.218613 1.243518 1.286120 \n\n\nThere is a 50% chance that the population SD of salary is between \\(e^{1.17}\\) and \\(e^{1.22}\\) dollars. There is an 80% chance that the population SD of salary is between \\(e^{1.14}\\) and \\(e^{1.25}\\) dollars. There is an 98% chance that the population SD of salary is between \\(e^{1.11}\\) and \\(e^{1.29}\\) dollars.\n\\(e^{1.11}\\) is roughly around 3.03 dollars and \\(e^{1.29}\\) is roughly around 3.63 dollars. This is a little strange, but since we transformed our response, the standard deviation likely wasn’t as varied transformed.\n\n\nStephen Curry Posterior Analysis (Model 1)\n\nposterior_steph = fit |>\n  spread_draws(b_Intercept, b_MP, b_USG., `b_MP:USG.`, sigma) |>\n  mutate(Steph_y = b_Intercept + \n           b_MP * 34.7 + \n           b_USG. * 31.0 + \n           `b_MP:USG.` * 34.7 * 31.0)\n\nposterior_steph |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_MP:USG. \n    sigma \n    Steph_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.93006 \n    0.0370065 \n    -0.0493851 \n    0.0033027 \n    1.170048 \n    17.23599 \n  \n  \n    1 \n    2 \n    2 \n    14.02398 \n    0.0171735 \n    -0.0462256 \n    0.0039307 \n    1.143928 \n    17.41518 \n  \n  \n    1 \n    3 \n    3 \n    14.19821 \n    0.0382640 \n    -0.0635444 \n    0.0032073 \n    1.210008 \n    17.00620 \n  \n  \n    1 \n    4 \n    4 \n    13.87291 \n    0.0388952 \n    -0.0278952 \n    0.0026233 \n    1.177849 \n    17.17972 \n  \n  \n    1 \n    5 \n    5 \n    14.62475 \n    -0.0037177 \n    -0.0744778 \n    0.0046737 \n    1.207042 \n    17.21441 \n  \n  \n    1 \n    6 \n    6 \n    14.61113 \n    -0.0035954 \n    -0.0732371 \n    0.0046510 \n    1.208182 \n    17.21910 \n  \n  \n    1 \n    7 \n    7 \n    13.71179 \n    0.0440438 \n    -0.0294713 \n    0.0027034 \n    1.202079 \n    17.23456 \n  \n  \n    1 \n    8 \n    8 \n    13.28116 \n    0.0675191 \n    -0.0065154 \n    0.0015795 \n    1.162212 \n    17.12118 \n  \n  \n    1 \n    9 \n    9 \n    14.10877 \n    0.0239218 \n    -0.0450594 \n    0.0033476 \n    1.172498 \n    17.14304 \n  \n  \n    1 \n    10 \n    10 \n    14.37647 \n    0.0260230 \n    -0.0637439 \n    0.0035893 \n    1.240193 \n    17.16445 \n  \n\n\n\n\n\n\nposterior_steph |>\n  ggplot(aes(x = Steph_y)) +\n  stat_halfeye(.width = c(0.80, 0.98),\n               fill = bayes_col[\"posterior\"]) +\n  theme_bw()\n\n\n\n\n\nquantile(posterior_steph$Steph_y, c(0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99))\n\n      1%      10%      25%      50%      75%      90%      99% \n16.75231 16.92697 17.03057 17.14424 17.25532 17.36493 17.53746 \n\n\nThere is a 50% chance that the salary of a player with Stephen Curry’s USG and MP is between \\(e^{17.031}\\) and \\(e^{17.258}\\) dollars. There is an 80% chance that the salary of a player with Stephen Curry’s USG and MP is between \\(e^{16.926}\\) and \\(e^{17.361}\\) dollars. There is a 98% chance that the salary of a player with Stephen Curry’s USG and MP is between \\(e^{16.747}\\) and \\(e^{17.533}\\) dollars.\nIn other words for the 98% credible interval, there is a 98% chance that the salary of a player with Stephen Curry’s USG and MP is between 18,755,545 and 41,160,927 dollars. This is a really accurate prediction, especially since Stephen Curry’s salaries is the highest in the data set.\n\n\nDavion Mitchell Posterior Analysis (Model 1)\nWe then inspected a player in the middle of the list, Davion Mitchell who plays a decent amount of games and is a reasonably average NBA player to analyze.\n\nposterior_davion = fit |>\n  spread_draws(b_Intercept, b_MP, b_USG., `b_MP:USG.`, sigma) |>\n  mutate(Davion_y = b_Intercept + \n           b_MP * 14.1 + \n           b_USG. * 31.0 + \n           `b_MP:USG.` * 14.1 * 31.0)\n\nposterior_davion |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_MP:USG. \n    sigma \n    Davion_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.93006 \n    0.0370065 \n    -0.0493851 \n    0.0033027 \n    1.170048 \n    14.36454 \n  \n  \n    1 \n    2 \n    2 \n    14.02398 \n    0.0171735 \n    -0.0462256 \n    0.0039307 \n    1.143928 \n    14.55125 \n  \n  \n    1 \n    3 \n    3 \n    14.19821 \n    0.0382640 \n    -0.0635444 \n    0.0032073 \n    1.210008 \n    14.16977 \n  \n  \n    1 \n    4 \n    4 \n    13.87291 \n    0.0388952 \n    -0.0278952 \n    0.0026233 \n    1.177849 \n    14.70324 \n  \n  \n    1 \n    5 \n    5 \n    14.62475 \n    -0.0037177 \n    -0.0744778 \n    0.0046737 \n    1.207042 \n    14.30638 \n  \n  \n    1 \n    6 \n    6 \n    14.61113 \n    -0.0035954 \n    -0.0732371 \n    0.0046510 \n    1.208182 \n    14.32304 \n  \n  \n    1 \n    7 \n    7 \n    13.71179 \n    0.0440438 \n    -0.0294713 \n    0.0027034 \n    1.202079 \n    14.60086 \n  \n  \n    1 \n    8 \n    8 \n    13.28116 \n    0.0675191 \n    -0.0065154 \n    0.0015795 \n    1.162212 \n    14.72161 \n  \n  \n    1 \n    9 \n    9 \n    14.10877 \n    0.0239218 \n    -0.0450594 \n    0.0033476 \n    1.172498 \n    14.51247 \n  \n  \n    1 \n    10 \n    10 \n    14.37647 \n    0.0260230 \n    -0.0637439 \n    0.0035893 \n    1.240193 \n    14.33623 \n  \n\n\n\n\n\n\nposterior_davion |>\n  ggplot(aes(x = Davion_y)) +\n  stat_halfeye(.width = c(0.80, 0.98),\n               fill = bayes_col[\"posterior\"]) +\n  theme_bw()\n\n\n\n\n\nquantile(posterior_davion$Davion_y, c(0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99))\n\n      1%      10%      25%      50%      75%      90%      99% \n14.06535 14.25147 14.35474 14.46883 14.58527 14.69119 14.87951 \n\n\nThere is a 50% chance that the salary of a player with Davion Mitchell’s USG and MP is between \\(e^{14.354}\\) and \\(e^{14.590}\\) dollars. There is an 80% chance that the salary of a player with Davion Mitchell’s USG and MP is between \\(e^{14.250}\\) and \\(e^{14.698}\\) dollars. There is a 98% chance that the salary of a player with Davion Mitchell’s USG and MP is between \\(e^{14.070}\\) and \\(e^{14.866}\\) dollars.\nFor the 98% credible interval, there is a 98% chance that the salary of a player with Davion Mitchell’s USG and MP is between 1,289,803 and 2,859,050 dollars. This is a pretty accurate prediction to his actual salary.\n\n\nBRMS (Model 2)\nWe did another analysis below for both Stephen Curry and Davion Mitchell. We saw that Davion Mitchell had a lot more games played than Stephen Curry but had significantly lower pay. We wanted to see if games played had more of an effect on players like Davion, but would not players with higher stats like Curry.\nWill only look at the 98% credible interval for this analysis.\n\ntest_02 <- lm(log(Salary) ~ MP + USG. + GP + MP*USG., data = nba_salary)\nsummary(test_02)\n\n\nCall:\nlm(formula = log(Salary) ~ MP + USG. + GP + MP * USG., data = nba_salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8219 -0.6882  0.0414  0.6706  3.6089 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.7403724  0.3433410  40.020  < 2e-16 ***\nMP          -0.0377184  0.0172884  -2.182   0.0296 *  \nUSG.        -0.0441375  0.0177373  -2.488   0.0132 *  \nGP           0.0278838  0.0026632  10.470  < 2e-16 ***\nMP:USG.      0.0039952  0.0007361   5.428 9.24e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 462 degrees of freedom\nMultiple R-squared:  0.5233,    Adjusted R-squared:  0.5192 \nF-statistic: 126.8 on 4 and 462 DF,  p-value: < 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(test_02)\n\n\n\n\n\nfit_2 <- brm(data = nba_salary,\n           family = gaussian(),\n           log(Salary) ~ MP + USG. + GP + MP*USG.,\n           sample_prior = TRUE,\n           iter = 3500,\n           warmup = 1000,\n           chains = 4,\n           refresh = 0)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nsummary(fit_2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(Salary) ~ MP + USG. + GP + MP * USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.74      0.35    13.07    14.42 1.00     4531     5808\nMP           -0.04      0.02    -0.07    -0.00 1.00     4528     5541\nUSG.         -0.04      0.02    -0.08    -0.01 1.00     4761     5953\nGP            0.03      0.00     0.02     0.03 1.00     8467     7271\nMP:USG.       0.00      0.00     0.00     0.01 1.00     4451     5751\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.07      0.04     1.01     1.15 1.00     6798     6487\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ncolor_scheme_set(\"purple\")\n\npp_check(fit_2, ndraw = 100)\n\n\n\n\nA little more off than model 1 ppcheck.\n\npairs(fit_2)\n\n\n\n\nFrom this plot we can see that minutes played still has a positive correlation with the usage rate. Both minutes played and usage rate is independent of games played.\n\n\nStephen Curry Posterior Analysis (Model 2)\n\nposterior_steph_2 = fit_2 |>\n  spread_draws(b_Intercept, b_MP, b_USG., b_GP, `b_MP:USG.`, sigma) |>\n  mutate(Steph_y = b_Intercept + \n           b_MP * 34.7 + \n           b_USG. * 31.0 + \n           b_GP * 56 +\n           `b_MP:USG.` * 34.7 * 31.0)\n\nposterior_steph_2 |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_GP \n    b_MP:USG. \n    sigma \n    Steph_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.49536 \n    -0.0244795 \n    -0.0286339 \n    0.0265672 \n    0.0034560 \n    1.104117 \n    16.96370 \n  \n  \n    1 \n    2 \n    2 \n    14.19783 \n    -0.0642514 \n    -0.0664605 \n    0.0286436 \n    0.0050852 \n    1.064857 \n    16.98220 \n  \n  \n    1 \n    3 \n    3 \n    13.83181 \n    -0.0405200 \n    -0.0542585 \n    0.0287102 \n    0.0044288 \n    1.057402 \n    17.11557 \n  \n  \n    1 \n    4 \n    4 \n    13.91159 \n    -0.0210095 \n    -0.0533979 \n    0.0239638 \n    0.0038592 \n    1.042943 \n    17.02054 \n  \n  \n    1 \n    5 \n    5 \n    13.64288 \n    -0.0330182 \n    -0.0337974 \n    0.0264381 \n    0.0037901 \n    1.079796 \n    17.00698 \n  \n  \n    1 \n    6 \n    6 \n    13.50358 \n    -0.0324451 \n    -0.0336407 \n    0.0264968 \n    0.0040996 \n    1.079482 \n    17.22863 \n  \n  \n    1 \n    7 \n    7 \n    13.54291 \n    -0.0335147 \n    -0.0340889 \n    0.0301641 \n    0.0035574 \n    1.081017 \n    16.83906 \n  \n  \n    1 \n    8 \n    8 \n    13.91689 \n    -0.0436091 \n    -0.0505774 \n    0.0263818 \n    0.0043637 \n    1.053566 \n    17.00722 \n  \n  \n    1 \n    9 \n    9 \n    13.90655 \n    -0.0440917 \n    -0.0553922 \n    0.0297604 \n    0.0043289 \n    1.025806 \n    16.98257 \n  \n  \n    1 \n    10 \n    10 \n    13.78923 \n    -0.0493760 \n    -0.0417026 \n    0.0296250 \n    0.0042616 \n    1.039411 \n    17.02629 \n  \n\n\n\n\n\n\nposterior_steph_2 |>\n  ggplot(aes(x = Steph_y)) +\n  stat_halfeye(.width = c(0.80, 0.98),\n               fill = bayes_col[\"posterior\"]) +\n  theme_bw()\n\n\n\n\n\nquantile(posterior_steph_2$Steph_y, c(0.01, 0.99))\n\n      1%      99% \n16.56222 17.27539 \n\n\nThere is a 98% chance that the salary of a player with Stephen Curry’s USG, MP, and GP is between \\(e^{16.56}\\) and \\(e^{17.29}\\) dollars.\nThere is a 98% chance that a player with Stephen Curry’s stats will have a predicted salary of 15,556,669 to 32,130,940 dollars. This seems more incorrect as Curry has a salary closer to 40 million dollars, but reasonable when it comes to number of games played. Curry is an outlier in terms of salary, so this prediction makes sense.\n\n\nDavion Mitchell Posterior Analysis (Model 2)\n\nposterior_davion_2 = fit_2 |>\n  spread_draws(b_Intercept, b_MP, b_USG., b_GP, `b_MP:USG.`, sigma) |>\n  mutate(Davion_y = b_Intercept + \n           b_MP * 14.1 + \n           b_USG. * 31.0 + \n           b_GP * 80 +\n           `b_MP:USG.` * 14.1 * 31.0)\n\nposterior_davion_2 |> head(10) |> kbl() |> kable_styling()\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_GP \n    b_MP:USG. \n    sigma \n    Davion_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.49536 \n    -0.0244795 \n    -0.0286339 \n    0.0265672 \n    0.0034560 \n    1.104117 \n    15.89856 \n  \n  \n    1 \n    2 \n    2 \n    14.19783 \n    -0.0642514 \n    -0.0664605 \n    0.0286436 \n    0.0050852 \n    1.064857 \n    15.74583 \n  \n  \n    1 \n    3 \n    3 \n    13.83181 \n    -0.0405200 \n    -0.0542585 \n    0.0287102 \n    0.0044288 \n    1.057402 \n    15.81110 \n  \n  \n    1 \n    4 \n    4 \n    13.91159 \n    -0.0210095 \n    -0.0533979 \n    0.0239638 \n    0.0038592 \n    1.042943 \n    15.56398 \n  \n  \n    1 \n    5 \n    5 \n    13.64288 \n    -0.0330182 \n    -0.0337974 \n    0.0264381 \n    0.0037901 \n    1.079796 \n    15.90131 \n  \n  \n    1 \n    6 \n    6 \n    13.50358 \n    -0.0324451 \n    -0.0336407 \n    0.0264968 \n    0.0040996 \n    1.079482 \n    15.91492 \n  \n  \n    1 \n    7 \n    7 \n    13.54291 \n    -0.0335147 \n    -0.0340889 \n    0.0301641 \n    0.0035574 \n    1.081017 \n    15.98166 \n  \n  \n    1 \n    8 \n    8 \n    13.91689 \n    -0.0436091 \n    -0.0505774 \n    0.0263818 \n    0.0043637 \n    1.053566 \n    15.75204 \n  \n  \n    1 \n    9 \n    9 \n    13.90655 \n    -0.0440917 \n    -0.0553922 \n    0.0297604 \n    0.0043289 \n    1.025806 \n    15.84069 \n  \n  \n    1 \n    10 \n    10 \n    13.78923 \n    -0.0493760 \n    -0.0417026 \n    0.0296250 \n    0.0042616 \n    1.039411 \n    16.03299 \n  \n\n\n\n\n\n\nposterior_davion_2 |>\n  ggplot(aes(x = Davion_y)) +\n  stat_halfeye(.width = c(0.80, 0.98),\n               fill = bayes_col[\"posterior\"]) +\n  theme_bw()\n\n\n\n\n\nquantile(posterior_davion_2$Davion_y, c(0.01, 0.99))\n\n      1%      99% \n15.34616 16.28940 \n\n\nThere is a 98% chance that the salary of a player with Davion Mitchell’s USG, MP, and GP is between \\(e^{15.35}\\) and \\(e^{16.28}\\) dollars.\nIn other words, there is a 98% chance that a player with Davion Mitchell’s stats will have a predicted salary of 4,638,956 to 11,757,478 dollars. This seems more incorrect as Mitchell has a salary closer to 1 million dollars. Games played doesn’t mean a player gets paid more, but this model will over predicted players who have been in a lot of games. Davion Mitchell is an outlier in terms of the games played.\n\n\nConclusion\nBased on the credible intervals we obtained, our model only using minutes played and usage rate produces a closer prediction to the actual salary values. After including games played in our model, our predictions fell further from the actual numbers. Although, we attempted to include more variables, our predictions became more off because these variables don’t describe salary behavior as well.\nOne shortcoming we saw of our chosen model is that it isn’t not highly descriptive and doesn’t a describe the wide range of individual player statistics. For example is that certain players will have a lot of minutes played per game, has a high usage rate, but only one game played and not be paid over one million dollars. Their salary will be over predicted.\nThere is one point in Handout 27 Model Comparison that Professor Ross points out about a simple model versus a complex model. “However, we don’t always want to just choose the more complex model. Always choosing the more complex model over fits the data.” As we saw with our more complex model which included games played, it did over fit Davion Mitchell’s salary significance, despite being considered what would be an average player with an average players stats. So for this analysis we focused on minutes played (MP) and usage rate (USG), since we believed salaries would be affected the most significantly by these statistics and would describe salaries within the data set the best.\nOne thing we considered while concluding this report is the popularity of each player that was not mentioned in this data set. Many increases in salaries include outside variables such as endorsements, brand deals, and other marketability each player has. Player statistics is one aspect of salary, but outside influence could have much more effect and should be furthered analyzed.\nRelating and concluding our research with the Bayesian perspective, we found that our Bayesian analysis was valuable in our context since the salaries of players are constantly changing every season. It would be inaccurate to state our parameters without looking at the data. This Bayesian analysis approach allowed us to account for the always-changing landscape of basketball salaries.\n\n\nBRMS Untransformed Response Model\nDown below is our first model with no transformation. Although this model could be used, we found that a normal distribution in BRMS would’ve lead to an easier interpretation.\n\nfit_brms_prior_3 <- brm(data = nba_salary,\n           family = exponential(),\n           Salary ~ MP + USG.,\n           sample_prior = TRUE,\n           iter = 3500,\n           warmup = 1000,\n           chains = 4,\n           refresh = 0)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\ncolor_scheme_set(\"viridis\")\n\npar(mfrow = c(2, 2))\nplot(fit_brms_prior_3)\n\n\n\n\n\nsummary(fit_brms_prior_3)\n\n Family: exponential \n  Links: mu = log \nFormula: Salary ~ MP + USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.36      0.16    13.03    13.68 1.00    10337     7969\nMP            0.09      0.01     0.08     0.10 1.00     7383     7840\nUSG.          0.02      0.01     0.00     0.04 1.00     6700     7152\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ncolor_scheme_set(\"purple\")\n\npp_check(fit_brms_prior_3, ndraw = 100)\n\n\n\n\nSimilarly, simulated distributions follow a similar distribution with the actual curve. But again, the actual curve has some strange curves due to the salary.\n\nsummary(fit_brms_prior_3)\n\n Family: exponential \n  Links: mu = log \nFormula: Salary ~ MP + USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.36      0.16    13.03    13.68 1.00    10337     7969\nMP            0.09      0.01     0.08     0.10 1.00     7383     7840\nUSG.          0.02      0.01     0.00     0.04 1.00     6700     7152\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/STAT417Project/index.html",
    "href": "posts/STAT417Project/index.html",
    "title": "Cirrhosis Survival",
    "section": "",
    "text": "Evaluating the Impact of D-Penicillamine on Cirrhosis Survival\nA recent study examined whether D-penicillamine could improve survival in individuals diagnosed with primary biliary cirrhosis. The dataset included 418 participants, but 112 were excluded due to missing health data. Researchers analyzed survival time, measured as days from enrollment until death, with right-censoring applied to those who either lived beyond the study or received a liver transplant. Key factors influencing survival included age, sex, bilirubin, hepatomegaly, copper, albumin, and SGOT levels. However, the drug itself showed no significant effect on mortality risk. These findings suggest that a patient’s overall health at enrollment is a stronger predictor of survival than the treatment, indicating that D-penicillamine is not an effective intervention for cirrhosis."
  },
  {
    "objectID": "posts/STAT545Project/index.html",
    "href": "posts/STAT545Project/index.html",
    "title": "Multiplayer Server Queue Simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(markovchain) # for plotting state diagram\nlibrary(igraph)\nlibrary(expm)\nlibrary(viridis)\nlibrary(viridisLite)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(prismatic) # for auto-contrast colors (black/numbers number in transition matrix heat map)\nlibrary(colourvalues) # for using with viridis\nlibrary(kableExtra)\nlibrary(reshape2)\n\n\n\nIn this project I aim to research more on the Jackson Network (Jacksonian Network) found in queuing theory. First I will discuss the theory and ideas behind Jackson Networks. Then I will use this in an actual application, where I will using a League of Legends as my example. League of Legends is a Multiplayer Online Battle Arena game (MOBA), where involves a total of 10 players for a match to begin, where each team has 5 players and both teams must work cooperatively to defeat the other team and take control of their side of the map. In this scenario I will be simulating individual players and the amount of time they spend in game, starting from the moment a player “queues” for a match from different regions in the world, where players will queue to choose one of seven different warrior classes, which vary in popularity in different regions. From there players can connect to three different servers, where some regions can connect to the region, but some regions cannot connect to others. This is meant to be a challenging premise, where there will be three sections of multiple queues. I plan to simulate multiple nodes. It will be an expanded version of the pictures below, found on Wikipedia from the Jackson Network page. I will be assuming this is an open Jackson Network, where there will be exogeneous inputs and will also depart at exogenous times as well. I want the queues to run at the same times, where times will be indepedent of other nodes / queues. The diagram on the LibreText also show a returning queue, but for my scenario I want to simplify one direction.\nSources\n\n\nhttps://eng.libretexts.org/Bookshelves/Electrical_Engineering/Signal_Processing_and_Modeling/Discrete_Stochastic_Processes_(Gallager)/06%3A_Markov_processes_with_countable_state_spaces/6.7%3A_Jackson_Networks\nWikipedia - Jackson Networks\nhttps://en.wikipedia.org/wiki/Jackson_network\n\n\n\n\n\nTo put it simply, a Jackson Network is a queuing system where customers process through different queues before leaving the system. Each queue is considered a node with an arrival rate and a service rate. In most cases outside nodes that are not interconnected in this network are considered exogenous arrivals (external arrivals). In a theoretical Jackson Network, customers enter one queue and decide if they want to leave or go to another queue and so on. Each queue follows a poisson arrival (\\(\\lambda\\)) and are serviced with an exponential rate (\\(\\mu\\)). A customers choice to do anything they want at any node are IID and independent of all service times, inputs, and other customer routing. Routing is assumed instantaneous. The network is a Markov process, where customers are in a state and state changes occur when exogenous arrivals and departures happen.\nThe balance equation of exogeneous arrival rates and services times and routing probabilities have to be consistent given by:\n\\[\n\\lambda_i Q_{ij} = \\lambda_j Q^*_{ji}\n\\]\nThe balance equation for a Jackson Network, considering all possible state transitions, is:\n\\[\n\\sum_{m'} q^*_{m, m'} = \\sum_{j=1}^{k} \\lambda_0 Q^*_{0j} + \\sum_{i: m_j > 0} \\mu_i Q^*_{i0} + \\sum_{i: m_j > 0} \\mu_i \\sum_{j=1}^{k} Q^*_{ij}\n\\]\nWhich is the sum of all possible transition from state m to another state m. Similar to something that I will create later. In other words, we can start by looking at steady-state distribution of the Jackson Network.\nThe same full balance equation is:\n\\[\n\\sum_{m'} q^*_{m, m'} = \\lambda_0 + \\sum_{i : m_i > 0} \\mu_i\n\\]\n\n\n\n\n\n\nThe queues will mostly follow an exponential distribution, where rates will be in minutes. The entire process should take nearly hours to finish a game. However, this simulation may not represent that these games do take long hours to finish. However, the main purpose of this project is to explore the ideas of Jackson Networks, even if this specific function may not work. According to\n\n\n\nFirst establish the region queues. I have six main regions with North America, South America, Europe, China, Korea, and Australia. Some of the main regions in this supped game, I will say each region will have their own expected probabilities for each of the character class. For example, I will say North America have a higher chance of choosing a Marksman over another region like South America.\n\n\n\nThere will be seven classes the queues will lead to. Like I stated earlier choosing one region will increase the probability of going to one queue to another. The seven classes are listed as follow. Bruiser, Fighter, Mage, Marksman, Slayer,Tank, Specialist. Each will be their own queue and have different rates to go on to the next server. Finally I plan to have three queues for the game, these games will plan to take an hour each. so really it should have a lambda of 1/60 for all queues. I simplified this problem to three queues to simulate the sample of players I am looking it. Imagine we are simulating if players join games in the same servers, and they come together from different regions and different classes.\n\nlambda_values_s <- c(\"NA\" = 1, \"SA\" = 1, \"KR\" = 2, \"CH\" = 2, \"EU\" = 0.5, \"AU\" = 0.5)\nmu_values_s <- c(\"NA\" = 0.6, \"SA\" = 0.75, \"KR\" = 0.5, \"CH\" = 0.5, \"EU\" = 0.6, \"AU\" = 0.8)\n\nmu_values_c <- c(\"Bruiser\" = 2, \"Fighter\" = 1, \"Mage\" = 1, \"Marksman\" = 0.5, \"Slayer\" = 0.2, \"Tank\" = 1, \"Specialist\" = 0.1)\n\nmu_values_g <- c(\"Game 1\" = 60, \"Game 2\" = 120, \"Game 3\" = 30)\nsd_values_g <- c(\"Game 1\" = 10, \"Game 2\" = 20, \"Game 3\" = 5)\n\nserver_indices <- c(\"NA\" = 1, \"SA\" = 2, \"KR\" = 3, \"CH\" = 4, \"EU\" = 5, \"AU\" = 6)\nclass_indices <- c(\"Bruiser\" = 1, \"Fighter\" = 2, \"Mage\" = 3, \"Marksman\" = 4, \"Slayer\" = 5, \"Tank\" = 6, \"Specialist\" = 7)\ngame_indices <- c(\"Game 1\" = 1, \"Game 2\" = 2, \"Game 3\" = 3)\n\n\nregion_to_class_probs <- matrix(\n  c(0.2, 0.1, 0.15, 0.2, 0.1, 0.15, 0.1,  # NA\n    0.15, 0.2, 0.1, 0.15, 0.2, 0.1, 0.1,  # SA\n    0.1, 0.15, 0.2, 0.1, 0.15, 0.2, 0.1,  # KR\n    0.2, 0.1, 0.1, 0.15, 0.15, 0.2, 0.1,  # CH\n    0.1, 0.2, 0.15, 0.1, 0.2, 0.15, 0.1,  # EU\n    0.15, 0.1, 0.2, 0.1, 0.1, 0.2, 0.15), # AU\n  nrow = 6, byrow = TRUE,\n  dimnames = list(c(\"NA\", \"SA\", \"KR\", \"CH\", \"EU\", \"AU\"), c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"))\n)\n\nclass_to_game_probs <- matrix(\n  c(0.0, 0.3, 0.7, # Bruiser\n    0.2, 0.3, 0.5, # Fighter \n    0.3, 0.4, 0.3, # Mage\n    0.0, 0.6, 0.4, # Marksman\n    0.5, 0.0, 0.5, # Slayer\n    0.5, 0.2, 0.3, # Tank\n    0.1, 0.9, 0.0), # Specialist\n  nrow = 7, byrow = TRUE,\n  dimnames = list(c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"), c(\"Game 1\", \"Game 2\", \"Game 3\"))\n)\n\nplayer_lim = 5\n  \nn_jumps = 100000\n\nregion_t = matrix(0, nrow = n_jumps + 1, ncol = 6)\nclass_t = matrix(0, nrow = n_jumps + 1, ncol = 7)\ngame_t = matrix(0, nrow = n_jumps + 1, ncol = 3)\n\nW_n = matrix(0, nrow = n_jumps + 1, ncol = 3)\n\nW_n[1, ] = 0\n\nfor (n in 2:n_jumps){\n  server <- sample(c(\"NA\", \"SA\", \"KR\", \"CH\", \"EU\", \"AU\"), 1, prob = c(0.1, 0.05, 0.2, 0.3, 0.2, 0.05))\n  \n  lambda <- lambda_values_s[server]\n  mu <- mu_values_s[server]\n  \n  rate = lambda + sum(mu*pmin(player_lim, region_t[n - 1, server_indices[server]]))\n  service_rate = sum(mu*pmin(player_lim, region_t[n - 1, server_indices[server]]))\n  p_arrive = lambda / rate\n  p_service = service_rate / rate\n  decision <- sample(c(\"arrival\", \"service\"), size = 1, prob = c(p_arrive, p_service))\n  \n  region_t[n, ] <- region_t[n - 1, ]\n  class_t[n, ] <- class_t[n - 1, ]\n  game_t[n, ] <- game_t[n - 1, ]\n  \n  departing_player <- FALSE  # Track if someone leaves for next queue\n  \n  curr_ind <- server_indices[server]\n  \n  region_t[n, ] = region_t[n - 1, ]\n  \n  W_n[n, 1] = W_n[n - 1, 1] + rexp(1) / (lambda_values_s[server] + mu_values_s[server] * min(player_lim, region_t[n - 1, curr_ind]))\n  \n  if (decision == \"arrival\") {\n    region_t[n, curr_ind] = region_t[n - 1, curr_ind] + 1\n  } else {\n    region_t[n, curr_ind] <- region_t[n - 1, curr_ind] - 1\n    departing_player <- TRUE\n  }\n  \n  if (departing_player) {\n    class_selected <- sample(names(class_indices), \n                             1, \n                             prob = region_to_class_probs[server, ])\n    \n    class_ind <- class_indices[class_selected]\n    class_t[n, class_ind] <- class_t[n - 1, class_ind] + 1\n  }\n  \n  if (sum(class_t[n - 1, ] > 0) > 0)  {\n    active_classes <- names(class_indices)[class_t[n - 1, ] > 0]\n    \n    selected_class <- sample(active_classes, 1)\n    class_ind <- class_indices[selected_class]\n    \n    W_n[n, 2] = W_n[n - 1, 2] + rexp(1) / (1 + mu_values_c[class_ind] * min(player_lim, class_t[n - 1, class_ind]))\n    \n    # Lambda will be 1 once we immediately place it into the queue.\n    mu_class <- sum(mu*pmin(player_lim, class_t[n - 1, class_indices[class_ind]]))\n    service_prob <- mu_class / (mu_class + 1)\n    \n    if (runif(1) < service_prob) {\n      class_t[n, class_ind] <- class_t[n - 1, class_ind] - 1\n      game_selected <- sample(names(game_indices), \n                             1, \n                             prob = class_to_game_probs[class_ind, ])\n      game_ind <- game_indices[game_selected]\n      game_t[n, game_ind] <- class_t[n - 1, game_ind] + 1\n      } \n    } else {\n      W_n[n, 2] = W_n[n - 1, 2]\n    }\n  \n  if (sum(game_t[n - 1, ] > 0) > 0) {\n    active_games <- names(game_indices)[game_t[n - 1, ] > 0]\n    \n    selected_game <- sample(active_games, 1)\n    game_ind <- game_indices[selected_game]\n    \n    W_n[n, 3] <- W_n[n - 1, 3] + rnorm(1, \n                                       mean = mu_values_g[selected_game], \n                                       sd = sd_values_g[selected_game]) / \n  (1 + mu_values_g[selected_game] * min(player_lim, game_t[n - 1, game_ind]))\n    \n    mu_game <- mu_values_g[selected_game]\n    service_prob <- mu_game / (mu_game + 1)\n    \n    if (runif(1) < service_prob) {\n      game_t[n, game_ind] <- game_t[n - 1, game_ind] - 1\n    }\n  } else {\n    W_n[n, 3] <- W_n[n - 1, 3]\n  }\n\n}\n\nregion_t <- na.omit(region_t)\nclass_t <- na.omit(class_t)\ngame_t <- na.omit(game_t)\nW_n <- na.omit(W_n)\n\n\n\n\nSimilar to how the Costco Time Application functions, I wanted this project and function to work in a similar manner. The struggle is connecting three individual queues and gathering times for those to work. Considerably, one of the biggest concerns I had was the number of short exponential times that I need to combine. While this function might not be necessarily correct in how it functions and it could be written more correct, I wanted to focus more on the ideas of Jacksonian Networks. First this paragraph will describe what is different from this code to the Costco application.\nThe first major change is that all queues have their own lambda and mu’s. Which are not exclusive to just one rate. This will be more apparent when I produce graphs of how many players are in each queue. For this specifically simulation, I’m exaggerating how Asian countries like China and South Korea have more players. This is more true in the real world of League of Legends, where a majority of players are from these specific countries, but they have many more servers and countries than this simplified version.\nThe second major change in this project is that I have created a transition matrix between queues. I was surprised on how this works, but it made sense to figure out where the next transition in what state would lead to another. For the regions to class transition matrix, I wanted all regions to have a transition to another queue, so no cell is set to 0. However, for the class to game indices I figured some class types could be potentially “banned” from the simplified three games I had running at the time.\n\n\nI did try and implement a tracking for each individual players, however this code wouldn’t work. For a future project if I look back at this I would try and implement this so I can have individual. So for now, in the scope of this project. I want to focus on distribution of the number of players in each queue. I also do not know if my implementation of a normal distribution for the time works. I lose the memoryless property by changing this to a normal distribution. However, I think the times for these ones are reasonable. Some times the implementation to the second queue can add two to the next queue, however I am very happy with the results. My times for each one may be incorrect and from the LibreTexts, the service times at all other nodes are independent of exogenous arrival times at all nodes.\n\nregion_c_total <- data.frame(colMeans(region_t, na.rm = TRUE))\ncolnames(region_c_total)[1] <- \"count\"\nregion_c_total$Region <- c(\"North America\", \"South America\", \"Korea\", \"China\", \"Europe\", \"Australia\")\n\nclass_c_total <- data.frame(colMeans(class_t, na.rm = TRUE))\ncolnames(class_c_total)[1] <- \"count\"\nclass_c_total$Class <- c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\")\n\ngame_c_total <- data.frame(colMeans(game_t, na.rm = TRUE))\ncolnames(game_c_total)[1] <- \"count\"\ngame_c_total$Game <- c(\"Game 1\", \"Game 2\", \"Game 3\")\n\nregion_r_total <- rowSums(region_t, na.rm = TRUE)\nclass_r_total <- rowSums(class_t, na.rm = TRUE)\ngame_r_total <- rowSums(game_t, na.rm = TRUE)\ntotal_df <- data.frame(\n  t = W_n,\n  region_total = region_r_total,\n  class_total = class_r_total,\n  game_total = game_r_total\n)\n\n\nggplot(region_c_total, aes(x = Region, y = count, fill = Region)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Region Queue\",\n       x = \"Region\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\nregion_df <- data.frame(t.1 = total_df$t.1, region_total = total_df$region_total)\n\nggplot(region_df, aes(x = t.1, y = region_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Region Queue Total\", title = \"Region Queue Total Over Time\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nregion_t_df <- as.data.frame(region_t)\nregion_t_df$total <- rowSums(region_t_df, na.rm = TRUE)\nregion_t_df$t <- W_n[,1]\n\ncol_plot <- paste0(\"V\", 1:ncol(region_t))\nd_long_1 <- melt(region_t_df[, c(\"t\", col_plot)], id.vars = \"t\")\nd_long_1$variable <- factor(d_long_1$variable, levels = col_plot, labels = c(\"North America\", \"South America\", \"Korea\", \"China\", \"Europe\", \"Australia\"))\n\nd_long_1 <- na.omit(d_long_1)\n\nggplot(d_long_1, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Region Total\", color = \"Region\", title = \"Sample Path of Regions Over Time\")\n\n\n\n\n\n\n\n\nHere in Region, Korea and China will have the largest queues, but also one of the fastest processing queues. In the total queue, it never goes near a point near 0 players in the queue, but it does stabilize around 30 players in the queue on average. By these numbers, thousands of players will have gone through the queue in a near 10 hour span. From the sample paths over the regions, China and Korea once again have one of the most volatile queues, spiking near 30 in the queue by itself, while the other queues stay relatively low. I wanted the other countries to not have as many, because in the real game, the queues from other servers are not as long as those servers. You also need Chinese or Korean I.D. to access these servers, so it’s a surprise how large these queues can actually become.\n\nggplot(class_c_total, aes(x = Class, y = count, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Class Queue\",\n       x = \"Class\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\nclass_df <- data.frame(t.2 = total_df$t.2, class_total = total_df$class_total)\n\nggplot(class_df, aes(x = t.2, y = class_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Class Queue Total\", title = \"Class Queue Total Over Time\")\n\n\n\n\n\nclass_t_df <- as.data.frame(class_t)\nclass_t_df$total <- rowSums(class_t_df, na.rm = TRUE)\nclass_t_df$t <- W_n[,2]\n\ncol_plot_class <- paste0(\"V\", 1:ncol(class_t))\nd_long_class <- melt(class_t_df[, c(\"t\", col_plot_class)], id.vars = \"t\")\nd_long_class$variable <- factor(d_long_class$variable, levels = col_plot_class, labels = c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"))\n\nd_long_class <- na.omit(d_long_class)\n\nggplot(d_long_class, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Class Total\", color = \"Class\", title = \"Sample Path of Classes Over Time\")\n\n\n\n\n\n\n\nClass queues shouldn’t have that many players in each queue. The only concerning ones should be bruiser and fighters, which do spike. But most of the classes are relatively equal. Specialist should have the lowest queue, as most players do not play this class specifically. But overall this graph seems representative of an actual League of Legends players journey. The largest bottle neck of this process should be connecting to the servers and the actual game itself which takes nearly an hour each. Tanks do show up at the top as well with the highest queues but the class queues themselves are very quick as well.\n\nggplot(game_c_total, aes(x = Game, y = count, fill = Game)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Game Queue\",\n       x = \"Game\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\ngame_df <- data.frame(t.3 = total_df$t.3, game_total = total_df$game_total)\n\nggplot(game_df, aes(x = t.3, y = game_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Game Total\", title = \"Class Game Total Over Time\")\n\n\n\n\n\ngame_t_df <- as.data.frame(game_t)\ngame_t_df$total <- rowSums(game_t_df, na.rm = TRUE)\ngame_t_df$t <- W_n[,3]\n\ncol_plot_game <- paste0(\"V\", 1:ncol(game_t))\nd_long_game <- melt(game_t_df[, c(\"t\", col_plot_game)], id.vars = \"t\")\nd_long_game$variable <- factor(d_long_game$variable, levels = col_plot_game, labels = c(\"Game 1\", \"Game 2\", \"Game 3\"))\n\nd_long_game <- na.omit(d_long_game)\n\nggplot(d_long_game, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Game Total\", color = \"Game\", title = \"Sample Path of Games Over Time\")\n\n\n\n\n\n\n\nIt’s surprising to see Game 2 being so high with the queues. Again I expected to see a lot of volatility from this queue because the times are following a normal distribution instead. Game 3 having the shortest queues also makes sense, because the normal distribution it follows is way shorter than Game 2’s queue. Game 1 is in the middle and shows that the normal distribution affecting the queues does work.\n\n\nAgain, I believed that the normal distribution was implemented correctly here. But it could be that it wasn’t correct and the transition matrix is working well. Unfortunately I don’t know the theory behind that to work yet.\n\n\n\n\n\nI hope my analytics and my simulation was enough exploration for the scope of this class. I believe my code and output are mostly correct with appropriate changes that go outside of the class. When thinking about Jacksonian Networks, I wasn’t expecting to use the transition matrices when changing from queue to queue. The discussion are correct because of the way that Is et them up. And while my code and premise do not align with what it sates on LibreText and Wikipedia. I felt that when I was taking the entirety of STAT 545, I didn’t understand the use of learning stochastic processes until I started reading about queuing theory. Especially when it comes to the work place and management and Jackson Networks. It’s obvious how management science uses this concept a lot and how they rely on graphs like the ones above. Not only could this work in queues, but call centers, product lines, DMV lines, and so much more. research into other areas I didn’t think could be useful to apply towards."
  },
  {
    "objectID": "posts/STAT551Project/index.html",
    "href": "posts/STAT551Project/index.html",
    "title": "School Modality Project",
    "section": "",
    "text": "School Modality Project\nOur School Modality Project examines the factors that influenced school districts’ decisions to adopt in-person, hybrid, or remote learning during the 2020-2021 school year. Using publicly available data from the CDC and school finance databases, we analyzed variables such as state, per-student spending, student enrollment, poverty rate, and demographics. Our findings indicate that districts with a higher proportion of white students were more likely to remain in-person, while larger districts and those with higher per-student spending tended to adopt hybrid or remote learning. Funding played a significant role in these decisions, as predominantly white districts received more resources, and higher spending facilitated smoother transitions to online learning through better access to technology. While our analysis provides valuable insights, we acknowledge ethical considerations such as missing IEP data, unclear terminology, and the need for equitable resource distribution. Ultimately, our project highlights how financial disparities and district size shaped learning modality decisions, emphasizing the importance of funding in determining educational access during the pandemic."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "SAT Statewide Categorical Analysis Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nAdam Kong, Sabrina Ahrendt, Franchesca Garcia, Julius Hoffman, Alex Tran\n\n\n\n\n\n\n  \n\n\n\n\nMultiplayer Server Queue Simulation\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSchool Modality Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\nAdam Kong, Alisa Krasilnikov, Dylan Le\n\n\n\n\n\n\n  \n\n\n\n\nBrawl Stars Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSushi DMAIC Process Improvement\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSUPERCELL Game Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\n2020 World Series Research\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\n2023 Basketball Bayesian Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nAdam Kong, Cameron An\n\n\n\n\n\n\n  \n\n\n\n\nCirrhosis Survival\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nUS Honey Production Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAdam Kong, Alisa Krasilnikov, Harshini Karthikeyan\n\n\n\n\n\n\nNo matching items"
  }
]