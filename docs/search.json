[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adam Kong",
    "section": "",
    "text": "LinkedIn\n  \n  \n     GitHub\n  \n\n      \n\n\nHello! I am a Graduate from California Polytechnic State University in San Luis Obispo. I graduated with a degree in Statistics with a Computer Science Minor. My interest are in the Actuary and Insurance Fields. I am currently working on studying for the P Exam and working on personal projects. In my free time I love doing Sports Analysis and exercising.\n\n\n\nCalifornia Polytechnic State University, California | San Luis Obispo, CA\nB.S. in Statistics | Computer Science Minor | 2021 - 2025"
  },
  {
    "objectID": "posts/DATA301Project_1/index.html",
    "href": "posts/DATA301Project_1/index.html",
    "title": "2020 World Series Debate",
    "section": "",
    "text": "Was the Dodgersâ€™ 2020 World Series Win a Fluke? A Data-Driven Analysis\nUsing data from Lahmanâ€™s Baseball Database, I analyzed Major League Baseball trends from 1876 to 2020, focusing on wins, league performance, homeruns, and batting averages. Over time, the number of homeruns and total hits have increased, while batting averages have declinedâ€”likely due to improved pitching and defensive strategies.\nA key question in my analysis was whether the Dodgersâ€™ 2020 World Series win was a fluke, given the shortened season. Using Euclidean distances, I compared the 2020 Dodgers to past World Series winners. The results showed that 2020 was an outlier, with distances more than double those of a typical season, suggesting the shortened schedule played a significant role.\nWhile this doesnâ€™t discredit the Dodgersâ€™ talent, it does highlight how unique conditions in 2020 contributed to their championship. A fascinating example of how statistical analysis can shed light on sports narratives! âš¾ðŸ“Š #BaseballStats #MLBAnalysis #Dodgers\nWith their EPIC 2024 World Series Win, this is a very awesome and fitting project."
  },
  {
    "objectID": "posts/DATA301Project_2/index.html",
    "href": "posts/DATA301Project_2/index.html",
    "title": "SUPERCELL Game Analysis",
    "section": "",
    "text": "Competitive gaming has evolved beyond individual skillâ€”team dynamics and clan structures now play a crucial role in determining success. Supercellâ€™s trio of popular mobile gamesâ€”Clash Royale, Clash of Clans, and Brawl Starsâ€”each feature clan-based systems where players group together for strategic advantages. But how important is clan membership in determining a groupâ€™s ranking? Do the most successful clans always maintain full rosters? And how do these games compare in terms of player demographics and group behavior?\nUsing publicly available data from Supercellâ€™s APIs, I set out to explore these questions by analyzing clan structures, ranking trends, and regional distributions across the three games.\n\n\nInitially, I considered analyzing music data, but the API I explored proved cumbersome and less engaging than I had hoped. I wanted a dataset that was rich, dynamic, and relevant to a competitive online space. Supercellâ€™s APIs provided exactly thatâ€”real-time data on player rankings, clan structures, and global distributions.\nI focused on clan-based data for two key reasons. First, from a playerâ€™s perspective, clans are essential in these games, providing access to exclusive events, rewards, and competitive advantages. Second, I was curious about whether highly ranked clans always operated at full capacity or if some had more flexible membership policies.\nWhile one limitation of my analysis was the inability to merge datasets across games due to differing player IDs, this reflects a common challenge in real-world data scienceâ€”datasets rarely align perfectly, and preprocessing is often required to draw meaningful insights.\n\n\n\nTo conduct this analysis, I retrieved data from three Supercell APIs:\n\nBrawl Stars API\nClash Royale API\nClash of Clans API\n\nEach API provides JSON-formatted data that includes individual player statistics, clan memberships, ranking distributions, and regional data. I specifically extracted information related to clan sizes, rankings, and geographic distributions, allowing for a comparative analysis across the three games.\n\n\n\nOne of the most striking observations from the analysis is that top-ranking clans in all three games tend to have the maximum number of members. However, an interesting exception appears in Clash of Clans, where some high-ranking clans operate with as few as 38 membersâ€”well below the full capacity of 50. This suggests that while clan membership is valuable, some elite clans may prioritize quality over quantity, maintaining strict recruitment standards rather than simply filling available spots.\nIn contrast, Clash Royale exhibits a much more consistent patternâ€”nearly all top-ranking clans are at full capacity (50 members). This makes sense given the gameâ€™s structure, where clan participation directly influences rewards through events like Clan Wars. Brawl Stars, meanwhile, follows a similar trend, with most successful clubs operating near their member limit.\n\n\nAnother significant finding is the difference in regional player distributions across the games. The majority of randomly selected Clash Royale players did not disclose their location, suggesting a more globally distributed player base. Meanwhile, Clash of Clans players were more likely to be from Southeast Asia, indicating stronger regional engagement in specific markets.\nThe data also suggests that the most competitive clans, regardless of game, maintain active participation levels. While fluctuations in membership are common, the best-performing groups tend to remain near full capacity.\n\n\n\n\nTo ensure meaningful analysis, I capped my dataset at 500 observations, requiring that each clan have at least 10 members. This helped differentiate between active and inactive clans while keeping the dataset manageable.\nKey variables included:\n\nClan tag, name, type, and required trophies\nLocation data (region, country name, and country code)\nMembership count\n\nSince Clash of Clans and Clash Royale shared many of these attributes, they provided a useful basis for comparison. I also created a new â€œgameâ€ variable to track which game each data point came from.\nIn terms of data transformation, the only major adjustment involved grouping and unstacking membership counts to summarize averages. Beyond that, I avoided excessive summarization to retain as much original detail as possible.\n\n\n\nThis analysis reinforces the idea that successful clans in Supercell games tend to operate at or near full capacity, with slight variations depending on game mechanics. While Clash Royale and Brawl Stars see more rigidly structured, full-capacity top-tier clans, Clash of Clans exhibits more flexibility, allowing for high-ranking clans with smaller member counts.\nFrom a demographic perspective, Clash Royale appears to have a more globally dispersed player base, while Clash of Clans shows stronger regional concentrations, particularly in Southeast Asia.\nThe findings also highlight a broader takeaway about competitive gaming communitiesâ€”clan membership matters, but the exact nature of its importance varies depending on the structure of the game. In games where clans directly influence progression (such as Clash Royaleâ€™s Clan Wars), maintaining a full roster is crucial. In contrast, in games like Clash of Clans, where individual skill and attack strategies play a larger role, clans may operate effectively even with a slightly reduced membership.\n\n\n\n\nOpenAI. ChatGPT. 2024. https://chat.openai.com\nGoogle DeepMind. Gemini. 2024.\nTodd Motto. Public APIs. GitHub. https://github.com/toddmotto/public-apis\nSupercell. Brawl Stars API Documentation. https://developer.brawlstars.com/#/\nSupercell. Clash Royale API Documentation. https://developer.clashroyale.com/#/\nSupercell. Clash of Clans API Documentation. https://developer.clashofclans.com/#/"
  },
  {
    "objectID": "posts/DATA301Project_3/index.html",
    "href": "posts/DATA301Project_3/index.html",
    "title": "Brawl Stars Analysis",
    "section": "",
    "text": "Understanding Brawler Performance and Classification in Brawl Stars\nIn this analysis, I explored different machine learning models to classify brawler rarity and class in Brawl Stars. Using a k-nearest neighbors (KNN) approach, I found that the best-performing models used K = 49 for classifying rarity and K = 48 for classifying class. While decision trees could have been a viable alternative, I opted for KNN because it provided more specificity in predicting rarity and class without introducing additional complexity.\nI initially attempted K-means clustering and hierarchical clustering, but my program crashed due to excessive RAM usage. Given that I already knew the response variables, I realized that using unsupervised learning techniques was unnecessaryâ€”supervised methods were more appropriate for this classification task. In addition to classification, I also performed a regression analysis on predicted trophy counts for each brawler, first without explicitly naming the brawler and then including the brawler as a predictor.\nThe central question driving this analysis was whether a trend exists between player performance and brawler choice. Specifically, do players who use higher-rarity brawlersâ€”such as Legendary, Mythic, or Epicâ€”tend to perform better? The data suggests that these brawlers are indeed more commonly used, which aligns with expectations. However, the model struggled to accurately predict Damage Dealers and Supports, likely due to fewer examples of these classes in the dataset. This observation aligns with the gameâ€™s meta, where Artillery, Assassins, Controllers, Marksmen, and Tanks tend to be more mechanically engaging and viable in competitive play. In contrast, Damage Dealers and Supports often have lower survivability and fewer unique mechanics, making them less prevalent.\nThrough various classification models, I identified trophies, rarity, class, power, and the number of gadgets as key predictors for brawler classification. Notably, power level (maxed at 11) and the number of gadgets (maxed at 2) emerged as particularly relevant features.\nWhile the classification accuracy remained below 50%, the ability to effectively categorize 86 brawlers across six rarities and seven class types using only an API, Google Colab, and real-time data is a significant achievement. The continuously updating nature of the dataset makes this an ongoing and dynamic area of exploration, with potential for further refinement and deeper insights into player behavior and game balance."
  },
  {
    "objectID": "posts/ITP303Project/index.html",
    "href": "posts/ITP303Project/index.html",
    "title": "Sushi DMAIC Process Improvement",
    "section": "",
    "text": "Kikka Sushi, a leading provider of fresh sushi across the United States since 1986, operates in various businesses, supermarkets, restaurants, universities, and hospitals. It has locations in California Fresh Market, including San Luis Obispo, Pismo Beach, and Solvang. Sushi chefs are expected to quickly prepare high-quality sushi, along with special rolls and party platters.\nSushi rolling speed and quality are crucial to customer satisfaction. Rolling times of 1-2 minutes per roll and 5-7 minutes for packaging and display are ideal. Delays or inconsistencies in these processes lead to bottlenecks, material waste, and customer dissatisfaction. This project aims to improve the sushi rolling process using Lean Six Sigmaâ€™s DMAIC framework (Define, Measure, Analyze, Improve, Control).\nThe projectâ€™s initial findings identified that hand dexterity and ingredient placement order could improve rolling times. These small improvements, though not widely applicable across the organization, can significantly impact overall process efficiency. Recommendations include maintaining clean workstations, reducing distractions, training hand dexterity, and streamlining ingredient measurements."
  },
  {
    "objectID": "posts/STAT414Project/index.html",
    "href": "posts/STAT414Project/index.html",
    "title": "US Honey Production Project",
    "section": "",
    "text": "US Honey Production Analysis\nHoney production in the United States is influenced by multiple factors, including environmental conditions, colony numbers, and market prices. This study examines key predictors of honey production value using data from 1995 to 2021, focusing on whether these factors vary by state and over time. The data, sourced from the US Honey Production dataset on Kaggle, is structured hierarchically with two levels. The first level consists of yearly observations, including variables such as Colonies Number, Yield Per Colony, Production, Stocks, and Average Price, while the second level accounts for state-level factors like land mass. Additional variables that were not included in the original data set were temperature, grassland size, and agricultural field size which were considered but excluded due to redundancy and insignificance.\nTo analyze these factors, a mixed-effects model was applied. The model included fixed effects for colonies number, stocks, year, and their interaction, with an intercept of 3,075,000. The number of colonies had a significant positive effect (85.45), while stocks showed a negative association (-1.424). The interaction between stocks and year was also found to be statistically significant (0.1052). The results revealed substantial variability in honey production value across states, with state-level variance estimated at 7.68e+12. Additionally, the impact of colonies number varied by state, with a slope variance of 3,664. Despite these identified factors, a significant portion of the variability remained unexplained within states (5.59e+12). Overall, the model accounted for 57% of the total variation in honey production value.\nThe findings suggest that the number of colonies and stock levels are the strongest predictors of honey production value, with their effects varying across states and over time. This highlights the importance of localized factors in honey production. Future studies should explore additional environmental and economic variables to enhance predictive accuracy and provide deeper insights into the dynamics of honey production in the United States.\n\n\n\n\n\nEmbedded Slides"
  },
  {
    "objectID": "posts/STAT415Project/index.html",
    "href": "posts/STAT415Project/index.html",
    "title": "2023 Basketball Bayesian Analysis",
    "section": "",
    "text": "Our research question investigates how usage rate (USG) and minutes played per game (MP) affect an NBA playerâ€™s annual salary for the 2022-23 season. Usage rate is the percentage of a teamâ€™s possessions a player ends while on the court. Itâ€™s calculated by adding up a playerâ€™s field goal attempts, turnovers, and trips to the free throw line, and then dividing that by the teamâ€™s total of those plays when the player is on the court. Minutes played per game describes how many in-game minutes (there are 48 in-game minutes in a regular NBA game) a player averages per game.\nThis data set contains numerous variables for players in the NBA. We are using USG and MP (as well as their interaction) as explanatory variables and log(Salary) as the response variable.\nThe data set includes numerous variables for NBA players during the 2022-23 season. We decided to focus on the usage rate and minutes played per game statistics and how that affects a playerâ€™s salary. The data is publicly available because playersâ€™ salaries are reported by agents and reporters and in-game stats are often released by NBA stat keepers. Kaggle (where we got the data from) compiled the data set from numerous websites and databases.\n\nPre-Bayesian Analysis\nAs seen below these explanatory variables are not very normal. However, we went into our analysis by transforming the response variable of salaries since salaries are much more likely to be skewed to the right without previous insepction of the dataset, since a few select players will have extreme salaries.\n\n\n\n\n\nPossible transformations and inspection of data before analysis.\n\n\n\n\n\n\n\nProposed Bayesian Model\n\\[{\\log(Y)} = {\\beta_0} + {\\beta_1}(USG) + {\\beta_2}(MP) + {\\beta_3}(USG*MP) + {\\varepsilon}\\]\n\n\nChoice of Likelihood\nWe used a Normal likelihood to calculate the probability of observing a certain log(Salary) given certain parameters for the betas.\n\n\nAssumptions\nLinearity: The relationship between log(Salary) and the explanatory variables is linear Homoscedasticity: variance of the residuals is constant Independence of errors: Residuals are independent of each other Normally distributed: Residuals are approximately normally distributed Independence of independent variables: included interaction effect in model to see how variables interact with each other.\n\n\nBayesian Model Analysis\n\n\n\nCall:\nlm(formula = log(Salary) ~ MP + USG. + MP * USG., data = nba_salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8611 -0.5311  0.2033  0.7138  2.7588 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.1414029  0.3791128  37.301  < 2e-16 ***\nMP           0.0272010  0.0179315   1.517   0.1300    \nUSG.        -0.0485190  0.0197029  -2.463   0.0142 *  \nMP:USG.      0.0033111  0.0008146   4.065 5.65e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.191 on 463 degrees of freedom\nMultiple R-squared:  0.4103,    Adjusted R-squared:  0.4064 \nF-statistic: 107.4 on 3 and 463 DF,  p-value: < 2.2e-16\n\n\n\n\n\nWe used BRMS to choose a prior distribution for the data. The model chose a student_t(3, 15.1, 2.5) distribution for the intercept and a student_t(3, 0, 2.5) distribution for sigma. The MP and USG variables have a flat prior, which is an improper distribution, but one that makes no assumption of the distribution.\n\n\nBRMS Chosen Prior\n\n\n                   prior     class    coef group resp dpar nlpar lb ub\n                  (flat)         b                                    \n                  (flat)         b      MP                            \n                  (flat)         b MP:USG.                            \n                  (flat)         b    USG.                            \n student_t(3, 15.1, 2.5) Intercept                                    \n    student_t(3, 0, 2.5)     sigma                                0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n      default\n\n\n\n\nBRMS (First Model)\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nBRMS Posterior Output Analysis\nThe posterior distribution does not change very much if the prior changes. This is because we have a large sample size for our data, which means that the data is going to hold the most weight in the posterior distribution.\n\n\n\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(Salary) ~ MP + USG. + MP * USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    14.14      0.38    13.38    14.88 1.00     4292     4899\nMP            0.03      0.02    -0.01     0.06 1.00     4641     5394\nUSG.         -0.05      0.02    -0.09    -0.01 1.00     4516     5659\nMP:USG.       0.00      0.00     0.00     0.00 1.00     4291     5334\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.19      0.04     1.12     1.27 1.00     6202     6083\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote, that we did run an interaction between minutes played and player usage. We decided to keep that in when simulated because before simulation we found that the interaction was significant, but now it was not in the posterior. We did not find this out until after the simulation was ran, but we kept because itâ€™s notable that minutes played and usage, while correlated, itâ€™s interaction was not significant to the log salary.\n\n\n\n\n\nSimulated distributions follow a similar distribution with the actual curve. The actual curve has some strange curves due to the salary.\n\n\n\n\n\nFrom this plot we can see that minutes played has a positive correlation with the usage rate. This is expected as generally players with a high minutes played will have a larger usage rate.\n\n\n\n\n\nLooking good so far! The chains are very close to each other here.\n\n\nb_Intercept        b_MP      b_USG.       sigma \n  0.4291956   0.4640683   0.4516122   0.6083066 \n\n\n\n\n\n\n\nLooks good.\n\n\n\n\n\nMP is a positive coefficent, just like before the simulation.\n\n\n\n\n\n\n\n\n\n\n\nPossibly may have some negative simulated values, but more positive.\n\n\n\n\n\nSurprisingly the coefficient for Usage is negative. Not very strong as usage has a range from 0 to 100, and most players have below 50 so not as big of an effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    sigma \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.40485 \n    1.204247 \n  \n  \n    1 \n    2 \n    2 \n    13.34406 \n    1.219551 \n  \n  \n    1 \n    3 \n    3 \n    15.02886 \n    1.152280 \n  \n  \n    1 \n    4 \n    4 \n    14.75198 \n    1.173161 \n  \n  \n    1 \n    5 \n    5 \n    13.99844 \n    1.188551 \n  \n  \n    1 \n    6 \n    6 \n    14.09294 \n    1.199925 \n  \n  \n    1 \n    7 \n    7 \n    14.02182 \n    1.157511 \n  \n  \n    1 \n    8 \n    8 \n    13.98063 \n    1.149257 \n  \n  \n    1 \n    9 \n    9 \n    14.03309 \n    1.133600 \n  \n  \n    1 \n    10 \n    10 \n    13.97693 \n    1.207306 \n  \n\n\n\n\n\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    Intercept_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.40485 \n  \n  \n    1 \n    2 \n    2 \n    13.34406 \n  \n  \n    1 \n    3 \n    3 \n    15.02886 \n  \n  \n    1 \n    4 \n    4 \n    14.75198 \n  \n  \n    1 \n    5 \n    5 \n    13.99844 \n  \n  \n    1 \n    6 \n    6 \n    14.09294 \n  \n  \n    1 \n    7 \n    7 \n    14.02182 \n  \n  \n    1 \n    8 \n    8 \n    13.98063 \n  \n  \n    1 \n    9 \n    9 \n    14.03309 \n  \n  \n    1 \n    10 \n    10 \n    13.97693 \n  \n\n\n\n\n\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    sigma_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    1.204247 \n  \n  \n    1 \n    2 \n    2 \n    1.219551 \n  \n  \n    1 \n    3 \n    3 \n    1.152280 \n  \n  \n    1 \n    4 \n    4 \n    1.173161 \n  \n  \n    1 \n    5 \n    5 \n    1.188551 \n  \n  \n    1 \n    6 \n    6 \n    1.199925 \n  \n  \n    1 \n    7 \n    7 \n    1.157511 \n  \n  \n    1 \n    8 \n    8 \n    1.149257 \n  \n  \n    1 \n    9 \n    9 \n    1.133600 \n  \n  \n    1 \n    10 \n    10 \n    1.207306 \n  \n\n\n\n\n\n\n\nPosterior Population Analysis\n\n\n      1%      10%      25%      50%      75%      90%      99% \n13.27517 13.64588 13.88557 14.13771 14.39287 14.62084 15.02537 \n\n\nThere is a 50% chance that the population mean salary is between \\(e^{13.87}\\) and \\(e^{14.38}\\) dollars. There is an 80% chance that the population mean salary is between \\(e^{13.63}\\) and \\(e^{14.64}\\) dollars. There is a 98% chance that the population mean salary is between \\(e^{13.23}\\) and \\(e^{15.04}\\) dollars.\n\\(e^{13.23}\\) is roughly around 556,821.5 dollars and \\(e^{15.04}\\) is roughly around 3,402,429 dollars\n\n\n      1%      10%      25%      50%      75%      90%      99% \n1.110284 1.145708 1.167242 1.193142 1.219245 1.243345 1.286354 \n\n\nThere is a 50% chance that the population SD of salary is between \\(e^{1.17}\\) and \\(e^{1.22}\\) dollars. There is an 80% chance that the population SD of salary is between \\(e^{1.14}\\) and \\(e^{1.25}\\) dollars. There is an 98% chance that the population SD of salary is between \\(e^{1.11}\\) and \\(e^{1.29}\\) dollars.\n\\(e^{1.11}\\) is roughly around 3.03 dollars and \\(e^{1.29}\\) is roughly around 3.63 dollars. This is a little strange, but since we transformed our response, the standard deviation likely wasnâ€™t as varied transformed.\n\n\nStephen Curry Posterior Analysis (Model 1)\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_MP:USG. \n    sigma \n    Steph_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.40485 \n    0.0548780 \n    -0.0062152 \n    0.0017695 \n    1.204247 \n    17.01991 \n  \n  \n    1 \n    2 \n    2 \n    13.34406 \n    0.0545617 \n    0.0015346 \n    0.0015706 \n    1.219551 \n    16.97440 \n  \n  \n    1 \n    3 \n    3 \n    15.02886 \n    -0.0244940 \n    -0.0892599 \n    0.0056076 \n    1.152280 \n    17.44392 \n  \n  \n    1 \n    4 \n    4 \n    14.75198 \n    0.0045300 \n    -0.0867470 \n    0.0047307 \n    1.173161 \n    17.30883 \n  \n  \n    1 \n    5 \n    5 \n    13.99844 \n    0.0155831 \n    -0.0408150 \n    0.0035912 \n    1.188551 \n    17.13699 \n  \n  \n    1 \n    6 \n    6 \n    14.09294 \n    0.0178233 \n    -0.0452517 \n    0.0035047 \n    1.199925 \n    17.07861 \n  \n  \n    1 \n    7 \n    7 \n    14.02182 \n    0.0404170 \n    -0.0481537 \n    0.0028471 \n    1.157511 \n    16.99415 \n  \n  \n    1 \n    8 \n    8 \n    13.98063 \n    0.0323411 \n    -0.0431461 \n    0.0030347 \n    1.149257 \n    17.02974 \n  \n  \n    1 \n    9 \n    9 \n    14.03309 \n    0.0182546 \n    -0.0363864 \n    0.0034585 \n    1.133600 \n    17.25890 \n  \n  \n    1 \n    10 \n    10 \n    13.97693 \n    0.0369747 \n    -0.0420919 \n    0.0029937 \n    1.207306 \n    17.17546 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n      1%      10%      25%      50%      75%      90%      99% \n16.74413 16.92391 17.02689 17.14147 17.25778 17.36358 17.54615 \n\n\nThere is a 50% chance that the salary of a player with Stephen Curryâ€™s USG and MP is between \\(e^{17.031}\\) and \\(e^{17.258}\\) dollars. There is an 80% chance that the salary of a player with Stephen Curryâ€™s USG and MP is between \\(e^{16.926}\\) and \\(e^{17.361}\\) dollars. There is a 98% chance that the salary of a player with Stephen Curryâ€™s USG and MP is between \\(e^{16.747}\\) and \\(e^{17.533}\\) dollars.\nIn other words for the 98% credible interval, there is a 98% chance that the salary of a player with Stephen Curryâ€™s USG and MP is between 18,755,545 and 41,160,927 dollars. This is a really accurate prediction, especially since Stephen Curryâ€™s salaries is the highest in the data set.\n\n\nDavion Mitchell Posterior Analysis (Model 1)\nWe then inspected a player in the middle of the list, Davion Mitchell who plays a decent amount of games and is a reasonably average NBA player to analyze.\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_MP:USG. \n    sigma \n    Davion_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.40485 \n    0.0548780 \n    -0.0062152 \n    0.0017695 \n    1.204247 \n    14.75941 \n  \n  \n    1 \n    2 \n    2 \n    13.34406 \n    0.0545617 \n    0.0015346 \n    0.0015706 \n    1.219551 \n    14.84746 \n  \n  \n    1 \n    3 \n    3 \n    15.02886 \n    -0.0244940 \n    -0.0892599 \n    0.0056076 \n    1.152280 \n    14.36750 \n  \n  \n    1 \n    4 \n    4 \n    14.75198 \n    0.0045300 \n    -0.0867470 \n    0.0047307 \n    1.173161 \n    14.19449 \n  \n  \n    1 \n    5 \n    5 \n    13.99844 \n    0.0155831 \n    -0.0408150 \n    0.0035912 \n    1.188551 \n    14.52262 \n  \n  \n    1 \n    6 \n    6 \n    14.09294 \n    0.0178233 \n    -0.0452517 \n    0.0035047 \n    1.199925 \n    14.47335 \n  \n  \n    1 \n    7 \n    7 \n    14.02182 \n    0.0404170 \n    -0.0481537 \n    0.0028471 \n    1.157511 \n    14.34340 \n  \n  \n    1 \n    8 \n    8 \n    13.98063 \n    0.0323411 \n    -0.0431461 \n    0.0030347 \n    1.149257 \n    14.42557 \n  \n  \n    1 \n    9 \n    9 \n    14.03309 \n    0.0182546 \n    -0.0363864 \n    0.0034585 \n    1.133600 \n    14.67423 \n  \n  \n    1 \n    10 \n    10 \n    13.97693 \n    0.0369747 \n    -0.0420919 \n    0.0029937 \n    1.207306 \n    14.50199 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n      1%      10%      25%      50%      75%      90%      99% \n14.04659 14.23934 14.34862 14.47095 14.58620 14.69532 14.86884 \n\n\nThere is a 50% chance that the salary of a player with Davion Mitchellâ€™s USG and MP is between \\(e^{14.354}\\) and \\(e^{14.590}\\) dollars. There is an 80% chance that the salary of a player with Davion Mitchellâ€™s USG and MP is between \\(e^{14.250}\\) and \\(e^{14.698}\\) dollars. There is a 98% chance that the salary of a player with Davion Mitchellâ€™s USG and MP is between \\(e^{14.070}\\) and \\(e^{14.866}\\) dollars.\nFor the 98% credible interval, there is a 98% chance that the salary of a player with Davion Mitchellâ€™s USG and MP is between 1,289,803 and 2,859,050 dollars. This is a pretty accurate prediction to his actual salary.\n\n\nBRMS (Model 2)\nWe did another analysis below for both Stephen Curry and Davion Mitchell. We saw that Davion Mitchell had a lot more games played than Stephen Curry but had significantly lower pay. We wanted to see if games played had more of an effect on players like Davion, but would not players with higher stats like Curry.\nWill only look at the 98% credible interval for this analysis.\n\n\n\nCall:\nlm(formula = log(Salary) ~ MP + USG. + GP + MP * USG., data = nba_salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8219 -0.6882  0.0414  0.6706  3.6089 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.7403724  0.3433410  40.020  < 2e-16 ***\nMP          -0.0377184  0.0172884  -2.182   0.0296 *  \nUSG.        -0.0441375  0.0177373  -2.488   0.0132 *  \nGP           0.0278838  0.0026632  10.470  < 2e-16 ***\nMP:USG.      0.0039952  0.0007361   5.428 9.24e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 462 degrees of freedom\nMultiple R-squared:  0.5233,    Adjusted R-squared:  0.5192 \nF-statistic: 126.8 on 4 and 462 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log(Salary) ~ MP + USG. + GP + MP * USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.74      0.35    13.08    14.44 1.00     5001     6077\nMP           -0.04      0.02    -0.07    -0.00 1.00     4934     6130\nUSG.         -0.04      0.02    -0.08    -0.01 1.00     5182     5735\nGP            0.03      0.00     0.02     0.03 1.00     9039     6536\nMP:USG.       0.00      0.00     0.00     0.01 1.00     4958     6196\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.08      0.04     1.01     1.15 1.00     5666     5261\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\nA little more off than model 1 ppcheck.\n\n\n\n\n\nFrom this plot we can see that minutes played still has a positive correlation with the usage rate. Both minutes played and usage rate is independent of games played.\n\n\nStephen Curry Posterior Analysis (Model 2)\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_GP \n    b_MP:USG. \n    sigma \n    Steph_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.48283 \n    -0.0280019 \n    -0.0218425 \n    0.0275287 \n    0.0031478 \n    1.105768 \n    16.76171 \n  \n  \n    1 \n    2 \n    2 \n    13.52880 \n    -0.0241469 \n    -0.0386563 \n    0.0289255 \n    0.0033301 \n    1.073317 \n    16.69455 \n  \n  \n    1 \n    3 \n    3 \n    13.58264 \n    -0.0403084 \n    -0.0344208 \n    0.0270341 \n    0.0041277 \n    1.072027 \n    17.07094 \n  \n  \n    1 \n    4 \n    4 \n    14.04728 \n    -0.0512499 \n    -0.0657865 \n    0.0273223 \n    0.0049092 \n    1.083099 \n    17.04040 \n  \n  \n    1 \n    5 \n    5 \n    13.55265 \n    -0.0387647 \n    -0.0306456 \n    0.0302348 \n    0.0035624 \n    1.075760 \n    16.78275 \n  \n  \n    1 \n    6 \n    6 \n    13.40052 \n    -0.0267959 \n    -0.0228545 \n    0.0294279 \n    0.0031389 \n    1.068282 \n    16.78670 \n  \n  \n    1 \n    7 \n    7 \n    13.06423 \n    -0.0245535 \n    -0.0150045 \n    0.0319892 \n    0.0031422 \n    1.074816 \n    16.91849 \n  \n  \n    1 \n    8 \n    8 \n    13.78238 \n    -0.0338930 \n    -0.0551842 \n    0.0239274 \n    0.0043828 \n    1.049268 \n    16.95010 \n  \n  \n    1 \n    9 \n    9 \n    13.83966 \n    -0.0429355 \n    -0.0516866 \n    0.0286253 \n    0.0043719 \n    1.077175 \n    17.05337 \n  \n  \n    1 \n    10 \n    10 \n    13.82837 \n    -0.0422647 \n    -0.0483255 \n    0.0272944 \n    0.0044127 \n    1.076766 \n    17.13896 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n      1%      99% \n16.55984 17.27935 \n\n\nThere is a 98% chance that the salary of a player with Stephen Curryâ€™s USG, MP, and GP is between \\(e^{16.56}\\) and \\(e^{17.29}\\) dollars.\nThere is a 98% chance that a player with Stephen Curryâ€™s stats will have a predicted salary of 15,556,669 to 32,130,940 dollars. This seems more incorrect as Curry has a salary closer to 40 million dollars, but reasonable when it comes to number of games played. Curry is an outlier in terms of salary, so this prediction makes sense.\n\n\nDavion Mitchell Posterior Analysis (Model 2)\n\n\n\n\n \n  \n    .chain \n    .iteration \n    .draw \n    b_Intercept \n    b_MP \n    b_USG. \n    b_GP \n    b_MP:USG. \n    sigma \n    Davion_y \n  \n \n\n  \n    1 \n    1 \n    1 \n    13.48283 \n    -0.0280019 \n    -0.0218425 \n    0.0275287 \n    0.0031478 \n    1.105768 \n    15.98907 \n  \n  \n    1 \n    2 \n    2 \n    13.52880 \n    -0.0241469 \n    -0.0386563 \n    0.0289255 \n    0.0033301 \n    1.073317 \n    15.75960 \n  \n  \n    1 \n    3 \n    3 \n    13.58264 \n    -0.0403084 \n    -0.0344208 \n    0.0270341 \n    0.0041277 \n    1.072027 \n    15.91418 \n  \n  \n    1 \n    4 \n    4 \n    14.04728 \n    -0.0512499 \n    -0.0657865 \n    0.0273223 \n    0.0049092 \n    1.083099 \n    15.61687 \n  \n  \n    1 \n    5 \n    5 \n    13.55265 \n    -0.0387647 \n    -0.0306456 \n    0.0302348 \n    0.0035624 \n    1.075760 \n    16.03197 \n  \n  \n    1 \n    6 \n    6 \n    13.40052 \n    -0.0267959 \n    -0.0228545 \n    0.0294279 \n    0.0031389 \n    1.068282 \n    16.04046 \n  \n  \n    1 \n    7 \n    7 \n    13.06423 \n    -0.0245535 \n    -0.0150045 \n    0.0319892 \n    0.0031422 \n    1.074816 \n    16.18546 \n  \n  \n    1 \n    8 \n    8 \n    13.78238 \n    -0.0338930 \n    -0.0551842 \n    0.0239274 \n    0.0043828 \n    1.049268 \n    15.42370 \n  \n  \n    1 \n    9 \n    9 \n    13.83966 \n    -0.0429355 \n    -0.0516866 \n    0.0286253 \n    0.0043719 \n    1.077175 \n    15.83296 \n  \n  \n    1 \n    10 \n    10 \n    13.82837 \n    -0.0422647 \n    -0.0483255 \n    0.0272944 \n    0.0044127 \n    1.076766 \n    15.84671 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n      1%      99% \n15.34250 16.28187 \n\n\nThere is a 98% chance that the salary of a player with Davion Mitchellâ€™s USG, MP, and GP is between \\(e^{15.35}\\) and \\(e^{16.28}\\) dollars.\nIn other words, there is a 98% chance that a player with Davion Mitchellâ€™s stats will have a predicted salary of 4,638,956 to 11,757,478 dollars. This seems more incorrect as Mitchell has a salary closer to 1 million dollars. Games played doesnâ€™t mean a player gets paid more, but this model will over predicted players who have been in a lot of games. Davion Mitchell is an outlier in terms of the games played.\n\n\nConclusion\nBased on the credible intervals we obtained, our model only using minutes played and usage rate produces a closer prediction to the actual salary values. After including games played in our model, our predictions fell further from the actual numbers. Although, we attempted to include more variables, our predictions became more off because these variables donâ€™t describe salary behavior as well.\nOne shortcoming we saw of our chosen model is that it isnâ€™t not highly descriptive and doesnâ€™t a describe the wide range of individual player statistics. For example is that certain players will have a lot of minutes played per game, has a high usage rate, but only one game played and not be paid over one million dollars. Their salary will be over predicted.\nThere is one point in Handout 27 Model Comparison that Professor Ross points out about a simple model versus a complex model. â€œHowever, we donâ€™t always want to just choose the more complex model. Always choosing the more complex model over fits the data.â€ As we saw with our more complex model which included games played, it did over fit Davion Mitchellâ€™s salary significance, despite being considered what would be an average player with an average players stats. So for this analysis we focused on minutes played (MP) and usage rate (USG), since we believed salaries would be affected the most significantly by these statistics and would describe salaries within the data set the best.\nOne thing we considered while concluding this report is the popularity of each player that was not mentioned in this data set. Many increases in salaries include outside variables such as endorsements, brand deals, and other marketability each player has. Player statistics is one aspect of salary, but outside influence could have much more effect and should be furthered analyzed.\nRelating and concluding our research with the Bayesian perspective, we found that our Bayesian analysis was valuable in our context since the salaries of players are constantly changing every season. It would be inaccurate to state our parameters without looking at the data. This Bayesian analysis approach allowed us to account for the always-changing landscape of basketball salaries.\n\n\nBRMS Untransformed Response Model\nDown below is our first model with no transformation. Although this model could be used, we found that a normal distribution in BRMS wouldâ€™ve lead to an easier interpretation.\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\n\n\n\n\n\n Family: exponential \n  Links: mu = log \nFormula: Salary ~ MP + USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.36      0.16    13.04    13.67 1.00    10679     7907\nMP            0.09      0.01     0.08     0.10 1.00     7050     6932\nUSG.          0.02      0.01     0.00     0.04 1.00     6878     6893\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\nSimilarly, simulated distributions follow a similar distribution with the actual curve. But again, the actual curve has some strange curves due to the salary.\n\n\n Family: exponential \n  Links: mu = log \nFormula: Salary ~ MP + USG. \n   Data: nba_salary (Number of observations: 467) \n  Draws: 4 chains, each with iter = 3500; warmup = 1000; thin = 1;\n         total post-warmup draws = 10000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    13.36      0.16    13.04    13.67 1.00    10679     7907\nMP            0.09      0.01     0.08     0.10 1.00     7050     6932\nUSG.          0.02      0.01     0.00     0.04 1.00     6878     6893\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/STAT417Project/index.html",
    "href": "posts/STAT417Project/index.html",
    "title": "Cirrhosis Survival Analysis",
    "section": "",
    "text": "Evaluating the Impact of D-Penicillamine on Cirrhosis Survival\nA recent study examined whether D-penicillamine could improve survival in individuals diagnosed with primary biliary cirrhosis. The dataset included 418 participants, but 112 were excluded due to missing health data. Researchers analyzed survival time, measured as days from enrollment until death, with right-censoring applied to those who either lived beyond the study or received a liver transplant. Key factors influencing survival included age, sex, bilirubin, hepatomegaly, copper, albumin, and SGOT levels. However, the drug itself showed no significant effect on mortality risk. These findings suggest that a patientâ€™s overall health at enrollment is a stronger predictor of survival than the treatment, indicating that D-penicillamine is not an effective intervention for cirrhosis."
  },
  {
    "objectID": "posts/STAT418Project/index.html",
    "href": "posts/STAT418Project/index.html",
    "title": "SAT Statewide Categorical Analysis Project",
    "section": "",
    "text": "SAT Categorical Analysis\nThis study aimed to identify key factors influencing state-level SAT performance in the United States using logistic regression. The primary focus was on predicting whether a state had an average SAT score above 925 (HighSAT), based on various demographic, financial, and educational variables. The dataset included factors like SAT participation rates (TAKERS), median family income (INCOME), years of formal education (YEARS), public school attendance (PUBLIC), state expenditure per student (EXPEND), and regional differences (REGION).\nKey findings from the analysis showed that factors such as TAKERS, PUBLIC, and EXPEND significantly contributed to high SAT performance. For instance, states with higher public school attendance and greater state expenditure per student were more likely to achieve a higher average SAT score. The study also encountered challenges with multicollinearity, especially between EXPEND and RANK, but through model selection, RANK was excluded from the final model.\nThe final model suggested that the percentage of students taking the SAT, the percentage attending public schools, and the amount spent on education per student were the most significant predictors of a state achieving a SAT average above 925. The study also provided a model for predicting the likelihood of a state achieving a high SAT score based on these variables.\nImprovements to the study could include breaking the SAT performance variable into more categories, incorporating individual-level data, and adding variables related to SAT preparation resources, which could further enhance the modelâ€™s accuracy."
  },
  {
    "objectID": "posts/STAT545Project/index.html",
    "href": "posts/STAT545Project/index.html",
    "title": "Multiplayer Server Queue Simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(markovchain) # for plotting state diagram\nlibrary(igraph)\nlibrary(expm)\nlibrary(viridis)\nlibrary(viridisLite)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(prismatic) # for auto-contrast colors (black/numbers number in transition matrix heat map)\nlibrary(colourvalues) # for using with viridis\nlibrary(kableExtra)\nlibrary(reshape2)\n\n\n\nIn this project I aim to research more on the Jackson Network (Jacksonian Network) found in queuing theory. First I will discuss the theory and ideas behind Jackson Networks. Then I will use this in an actual application, where I will using a League of Legends as my example. League of Legends is a Multiplayer Online Battle Arena game (MOBA), where involves a total of 10 players for a match to begin, where each team has 5 players and both teams must work cooperatively to defeat the other team and take control of their side of the map. In this scenario I will be simulating individual players and the amount of time they spend in game, starting from the moment a player â€œqueuesâ€ for a match from different regions in the world, where players will queue to choose one of seven different warrior classes, which vary in popularity in different regions. From there players can connect to three different servers, where some regions can connect to the region, but some regions cannot connect to others. This is meant to be a challenging premise, where there will be three sections of multiple queues. I plan to simulate multiple nodes. It will be an expanded version of the pictures below, found on Wikipedia from the Jackson Network page. I will be assuming this is an open Jackson Network, where there will be exogeneous inputs and will also depart at exogenous times as well. I want the queues to run at the same times, where times will be indepedent of other nodes / queues. The diagram on the LibreText also show a returning queue, but for my scenario I want to simplify one direction.\nSources\n\n\nhttps://eng.libretexts.org/Bookshelves/Electrical_Engineering/Signal_Processing_and_Modeling/Discrete_Stochastic_Processes_(Gallager)/06%3A_Markov_processes_with_countable_state_spaces/6.7%3A_Jackson_Networks\nWikipedia - Jackson Networks\nhttps://en.wikipedia.org/wiki/Jackson_network\n\n\n\n\n\nTo put it simply, a Jackson Network is a queuing system where customers process through different queues before leaving the system. Each queue is considered a node with an arrival rate and a service rate. In most cases outside nodes that are not interconnected in this network are considered exogenous arrivals (external arrivals). In a theoretical Jackson Network, customers enter one queue and decide if they want to leave or go to another queue and so on. Each queue follows a poisson arrival (\\(\\lambda\\)) and are serviced with an exponential rate (\\(\\mu\\)). A customers choice to do anything they want at any node are IID and independent of all service times, inputs, and other customer routing. Routing is assumed instantaneous. The network is a Markov process, where customers are in a state and state changes occur when exogenous arrivals and departures happen.\nThe balance equation of exogeneous arrival rates and services times and routing probabilities have to be consistent given by:\n\\[\n\\lambda_i Q_{ij} = \\lambda_j Q^*_{ji}\n\\]\nThe balance equation for a Jackson Network, considering all possible state transitions, is:\n\\[\n\\sum_{m'} q^*_{m, m'} = \\sum_{j=1}^{k} \\lambda_0 Q^*_{0j} + \\sum_{i: m_j > 0} \\mu_i Q^*_{i0} + \\sum_{i: m_j > 0} \\mu_i \\sum_{j=1}^{k} Q^*_{ij}\n\\]\nWhich is the sum of all possible transition from state m to another state m. Similar to something that I will create later. In other words, we can start by looking at steady-state distribution of the Jackson Network.\nThe same full balance equation is:\n\\[\n\\sum_{m'} q^*_{m, m'} = \\lambda_0 + \\sum_{i : m_i > 0} \\mu_i\n\\]\n\n\n\n\n\n\nThe queues will mostly follow an exponential distribution, where rates will be in minutes. The entire process should take nearly hours to finish a game. However, this simulation may not represent that these games do take long hours to finish. However, the main purpose of this project is to explore the ideas of Jackson Networks, even if this specific function may not work. According to\n\n\n\nFirst establish the region queues. I have six main regions with North America, South America, Europe, China, Korea, and Australia. Some of the main regions in this supped game, I will say each region will have their own expected probabilities for each of the character class. For example, I will say North America have a higher chance of choosing a Marksman over another region like South America.\n\n\n\nThere will be seven classes the queues will lead to. Like I stated earlier choosing one region will increase the probability of going to one queue to another. The seven classes are listed as follow. Bruiser, Fighter, Mage, Marksman, Slayer,Tank, Specialist. Each will be their own queue and have different rates to go on to the next server. Finally I plan to have three queues for the game, these games will plan to take an hour each. so really it should have a lambda of 1/60 for all queues. I simplified this problem to three queues to simulate the sample of players I am looking it. Imagine we are simulating if players join games in the same servers, and they come together from different regions and different classes.\n\nlambda_values_s <- c(\"NA\" = 1, \"SA\" = 1, \"KR\" = 2, \"CH\" = 2, \"EU\" = 0.5, \"AU\" = 0.5)\nmu_values_s <- c(\"NA\" = 0.6, \"SA\" = 0.75, \"KR\" = 0.5, \"CH\" = 0.5, \"EU\" = 0.6, \"AU\" = 0.8)\n\nmu_values_c <- c(\"Bruiser\" = 2, \"Fighter\" = 1, \"Mage\" = 1, \"Marksman\" = 0.5, \"Slayer\" = 0.2, \"Tank\" = 1, \"Specialist\" = 0.1)\n\nmu_values_g <- c(\"Game 1\" = 60, \"Game 2\" = 120, \"Game 3\" = 30)\nsd_values_g <- c(\"Game 1\" = 10, \"Game 2\" = 20, \"Game 3\" = 5)\n\nserver_indices <- c(\"NA\" = 1, \"SA\" = 2, \"KR\" = 3, \"CH\" = 4, \"EU\" = 5, \"AU\" = 6)\nclass_indices <- c(\"Bruiser\" = 1, \"Fighter\" = 2, \"Mage\" = 3, \"Marksman\" = 4, \"Slayer\" = 5, \"Tank\" = 6, \"Specialist\" = 7)\ngame_indices <- c(\"Game 1\" = 1, \"Game 2\" = 2, \"Game 3\" = 3)\n\n\nregion_to_class_probs <- matrix(\n  c(0.2, 0.1, 0.15, 0.2, 0.1, 0.15, 0.1,  # NA\n    0.15, 0.2, 0.1, 0.15, 0.2, 0.1, 0.1,  # SA\n    0.1, 0.15, 0.2, 0.1, 0.15, 0.2, 0.1,  # KR\n    0.2, 0.1, 0.1, 0.15, 0.15, 0.2, 0.1,  # CH\n    0.1, 0.2, 0.15, 0.1, 0.2, 0.15, 0.1,  # EU\n    0.15, 0.1, 0.2, 0.1, 0.1, 0.2, 0.15), # AU\n  nrow = 6, byrow = TRUE,\n  dimnames = list(c(\"NA\", \"SA\", \"KR\", \"CH\", \"EU\", \"AU\"), c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"))\n)\n\nclass_to_game_probs <- matrix(\n  c(0.0, 0.3, 0.7, # Bruiser\n    0.2, 0.3, 0.5, # Fighter \n    0.3, 0.4, 0.3, # Mage\n    0.0, 0.6, 0.4, # Marksman\n    0.5, 0.0, 0.5, # Slayer\n    0.5, 0.2, 0.3, # Tank\n    0.1, 0.9, 0.0), # Specialist\n  nrow = 7, byrow = TRUE,\n  dimnames = list(c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"), c(\"Game 1\", \"Game 2\", \"Game 3\"))\n)\n\nplayer_lim = 5\n  \nn_jumps = 100000\n\nregion_t = matrix(0, nrow = n_jumps + 1, ncol = 6)\nclass_t = matrix(0, nrow = n_jumps + 1, ncol = 7)\ngame_t = matrix(0, nrow = n_jumps + 1, ncol = 3)\n\nW_n = matrix(0, nrow = n_jumps + 1, ncol = 3)\n\nW_n[1, ] = 0\n\nfor (n in 2:n_jumps){\n  server <- sample(c(\"NA\", \"SA\", \"KR\", \"CH\", \"EU\", \"AU\"), 1, prob = c(0.1, 0.05, 0.2, 0.3, 0.2, 0.05))\n  \n  lambda <- lambda_values_s[server]\n  mu <- mu_values_s[server]\n  \n  rate = lambda + sum(mu*pmin(player_lim, region_t[n - 1, server_indices[server]]))\n  service_rate = sum(mu*pmin(player_lim, region_t[n - 1, server_indices[server]]))\n  p_arrive = lambda / rate\n  p_service = service_rate / rate\n  decision <- sample(c(\"arrival\", \"service\"), size = 1, prob = c(p_arrive, p_service))\n  \n  region_t[n, ] <- region_t[n - 1, ]\n  class_t[n, ] <- class_t[n - 1, ]\n  game_t[n, ] <- game_t[n - 1, ]\n  \n  departing_player <- FALSE  # Track if someone leaves for next queue\n  \n  curr_ind <- server_indices[server]\n  \n  region_t[n, ] = region_t[n - 1, ]\n  \n  W_n[n, 1] = W_n[n - 1, 1] + rexp(1) / (lambda_values_s[server] + mu_values_s[server] * min(player_lim, region_t[n - 1, curr_ind]))\n  \n  if (decision == \"arrival\") {\n    region_t[n, curr_ind] = region_t[n - 1, curr_ind] + 1\n  } else {\n    region_t[n, curr_ind] <- region_t[n - 1, curr_ind] - 1\n    departing_player <- TRUE\n  }\n  \n  if (departing_player) {\n    class_selected <- sample(names(class_indices), \n                             1, \n                             prob = region_to_class_probs[server, ])\n    \n    class_ind <- class_indices[class_selected]\n    class_t[n, class_ind] <- class_t[n - 1, class_ind] + 1\n  }\n  \n  if (sum(class_t[n - 1, ] > 0) > 0)  {\n    active_classes <- names(class_indices)[class_t[n - 1, ] > 0]\n    \n    selected_class <- sample(active_classes, 1)\n    class_ind <- class_indices[selected_class]\n    \n    W_n[n, 2] = W_n[n - 1, 2] + rexp(1) / (1 + mu_values_c[class_ind] * min(player_lim, class_t[n - 1, class_ind]))\n    \n    # Lambda will be 1 once we immediately place it into the queue.\n    mu_class <- sum(mu*pmin(player_lim, class_t[n - 1, class_indices[class_ind]]))\n    service_prob <- mu_class / (mu_class + 1)\n    \n    if (runif(1) < service_prob) {\n      class_t[n, class_ind] <- class_t[n - 1, class_ind] - 1\n      game_selected <- sample(names(game_indices), \n                             1, \n                             prob = class_to_game_probs[class_ind, ])\n      game_ind <- game_indices[game_selected]\n      game_t[n, game_ind] <- class_t[n - 1, game_ind] + 1\n      } \n    } else {\n      W_n[n, 2] = W_n[n - 1, 2]\n    }\n  \n  if (sum(game_t[n - 1, ] > 0) > 0) {\n    active_games <- names(game_indices)[game_t[n - 1, ] > 0]\n    \n    selected_game <- sample(active_games, 1)\n    game_ind <- game_indices[selected_game]\n    \n    W_n[n, 3] <- W_n[n - 1, 3] + rnorm(1, \n                                       mean = mu_values_g[selected_game], \n                                       sd = sd_values_g[selected_game]) / \n  (1 + mu_values_g[selected_game] * min(player_lim, game_t[n - 1, game_ind]))\n    \n    mu_game <- mu_values_g[selected_game]\n    service_prob <- mu_game / (mu_game + 1)\n    \n    if (runif(1) < service_prob) {\n      game_t[n, game_ind] <- game_t[n - 1, game_ind] - 1\n    }\n  } else {\n    W_n[n, 3] <- W_n[n - 1, 3]\n  }\n\n}\n\nregion_t <- na.omit(region_t)\nclass_t <- na.omit(class_t)\ngame_t <- na.omit(game_t)\nW_n <- na.omit(W_n)\n\n\n\n\nSimilar to how the Costco Time Application functions, I wanted this project and function to work in a similar manner. The struggle is connecting three individual queues and gathering times for those to work. Considerably, one of the biggest concerns I had was the number of short exponential times that I need to combine. While this function might not be necessarily correct in how it functions and it could be written more correct, I wanted to focus more on the ideas of Jacksonian Networks. First this paragraph will describe what is different from this code to the Costco application.\nThe first major change is that all queues have their own lambda and muâ€™s. Which are not exclusive to just one rate. This will be more apparent when I produce graphs of how many players are in each queue. For this specifically simulation, Iâ€™m exaggerating how Asian countries like China and South Korea have more players. This is more true in the real world of League of Legends, where a majority of players are from these specific countries, but they have many more servers and countries than this simplified version.\nThe second major change in this project is that I have created a transition matrix between queues. I was surprised on how this works, but it made sense to figure out where the next transition in what state would lead to another. For the regions to class transition matrix, I wanted all regions to have a transition to another queue, so no cell is set to 0. However, for the class to game indices I figured some class types could be potentially â€œbannedâ€ from the simplified three games I had running at the time.\n\n\nI did try and implement a tracking for each individual players, however this code wouldnâ€™t work. For a future project if I look back at this I would try and implement this so I can have individual. So for now, in the scope of this project. I want to focus on distribution of the number of players in each queue. I also do not know if my implementation of a normal distribution for the time works. I lose the memoryless property by changing this to a normal distribution. However, I think the times for these ones are reasonable. Some times the implementation to the second queue can add two to the next queue, however I am very happy with the results. My times for each one may be incorrect and from the LibreTexts, the service times at all other nodes are independent of exogenous arrival times at all nodes.\n\nregion_c_total <- data.frame(colMeans(region_t, na.rm = TRUE))\ncolnames(region_c_total)[1] <- \"count\"\nregion_c_total$Region <- c(\"North America\", \"South America\", \"Korea\", \"China\", \"Europe\", \"Australia\")\n\nclass_c_total <- data.frame(colMeans(class_t, na.rm = TRUE))\ncolnames(class_c_total)[1] <- \"count\"\nclass_c_total$Class <- c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\")\n\ngame_c_total <- data.frame(colMeans(game_t, na.rm = TRUE))\ncolnames(game_c_total)[1] <- \"count\"\ngame_c_total$Game <- c(\"Game 1\", \"Game 2\", \"Game 3\")\n\nregion_r_total <- rowSums(region_t, na.rm = TRUE)\nclass_r_total <- rowSums(class_t, na.rm = TRUE)\ngame_r_total <- rowSums(game_t, na.rm = TRUE)\ntotal_df <- data.frame(\n  t = W_n,\n  region_total = region_r_total,\n  class_total = class_r_total,\n  game_total = game_r_total\n)\n\n\nggplot(region_c_total, aes(x = Region, y = count, fill = Region)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Region Queue\",\n       x = \"Region\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\nregion_df <- data.frame(t.1 = total_df$t.1, region_total = total_df$region_total)\n\nggplot(region_df, aes(x = t.1, y = region_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Region Queue Total\", title = \"Region Queue Total Over Time\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n\n\n\n\nregion_t_df <- as.data.frame(region_t)\nregion_t_df$total <- rowSums(region_t_df, na.rm = TRUE)\nregion_t_df$t <- W_n[,1]\n\ncol_plot <- paste0(\"V\", 1:ncol(region_t))\nd_long_1 <- melt(region_t_df[, c(\"t\", col_plot)], id.vars = \"t\")\nd_long_1$variable <- factor(d_long_1$variable, levels = col_plot, labels = c(\"North America\", \"South America\", \"Korea\", \"China\", \"Europe\", \"Australia\"))\n\nd_long_1 <- na.omit(d_long_1)\n\nggplot(d_long_1, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Region Total\", color = \"Region\", title = \"Sample Path of Regions Over Time\")\n\n\n\n\n\n\n\n\nHere in Region, Korea and China will have the largest queues, but also one of the fastest processing queues. In the total queue, it never goes near a point near 0 players in the queue, but it does stabilize around 30 players in the queue on average. By these numbers, thousands of players will have gone through the queue in a near 10 hour span. From the sample paths over the regions, China and Korea once again have one of the most volatile queues, spiking near 30 in the queue by itself, while the other queues stay relatively low. I wanted the other countries to not have as many, because in the real game, the queues from other servers are not as long as those servers. You also need Chinese or Korean I.D. to access these servers, so itâ€™s a surprise how large these queues can actually become.\n\nggplot(class_c_total, aes(x = Class, y = count, fill = Class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Class Queue\",\n       x = \"Class\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\nclass_df <- data.frame(t.2 = total_df$t.2, class_total = total_df$class_total)\n\nggplot(class_df, aes(x = t.2, y = class_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Class Queue Total\", title = \"Class Queue Total Over Time\")\n\n\n\n\n\nclass_t_df <- as.data.frame(class_t)\nclass_t_df$total <- rowSums(class_t_df, na.rm = TRUE)\nclass_t_df$t <- W_n[,2]\n\ncol_plot_class <- paste0(\"V\", 1:ncol(class_t))\nd_long_class <- melt(class_t_df[, c(\"t\", col_plot_class)], id.vars = \"t\")\nd_long_class$variable <- factor(d_long_class$variable, levels = col_plot_class, labels = c(\"Bruiser\", \"Fighter\", \"Mage\", \"Marksman\", \"Slayer\", \"Tank\", \"Specialist\"))\n\nd_long_class <- na.omit(d_long_class)\n\nggplot(d_long_class, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Class Total\", color = \"Class\", title = \"Sample Path of Classes Over Time\")\n\n\n\n\n\n\n\nClass queues shouldnâ€™t have that many players in each queue. The only concerning ones should be bruiser and fighters, which do spike. But most of the classes are relatively equal. Specialist should have the lowest queue, as most players do not play this class specifically. But overall this graph seems representative of an actual League of Legends players journey. The largest bottle neck of this process should be connecting to the servers and the actual game itself which takes nearly an hour each. Tanks do show up at the top as well with the highest queues but the class queues themselves are very quick as well.\n\nggplot(game_c_total, aes(x = Game, y = count, fill = Game)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Players in Each Game Queue\",\n       x = \"Game\",\n       y = \"Average Players in Queue\")\n\n\n\n\n\ngame_df <- data.frame(t.3 = total_df$t.3, game_total = total_df$game_total)\n\nggplot(game_df, aes(x = t.3, y = game_total)) +\n  geom_line(size = 1) +\n  labs(x = \"Time\", y = \"Game Total\", title = \"Class Game Total Over Time\")\n\n\n\n\n\ngame_t_df <- as.data.frame(game_t)\ngame_t_df$total <- rowSums(game_t_df, na.rm = TRUE)\ngame_t_df$t <- W_n[,3]\n\ncol_plot_game <- paste0(\"V\", 1:ncol(game_t))\nd_long_game <- melt(game_t_df[, c(\"t\", col_plot_game)], id.vars = \"t\")\nd_long_game$variable <- factor(d_long_game$variable, levels = col_plot_game, labels = c(\"Game 1\", \"Game 2\", \"Game 3\"))\n\nd_long_game <- na.omit(d_long_game)\n\nggplot(d_long_game, aes(x = t, y = value, col = variable)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Game Total\", color = \"Game\", title = \"Sample Path of Games Over Time\")\n\n\n\n\n\n\n\nItâ€™s surprising to see Game 2 being so high with the queues. Again I expected to see a lot of volatility from this queue because the times are following a normal distribution instead. Game 3 having the shortest queues also makes sense, because the normal distribution it follows is way shorter than Game 2â€™s queue. Game 1 is in the middle and shows that the normal distribution affecting the queues does work.\n\n\nAgain, I believed that the normal distribution was implemented correctly here. But it could be that it wasnâ€™t correct and the transition matrix is working well. Unfortunately I donâ€™t know the theory behind that to work yet.\n\n\n\n\n\nI hope my analytics and my simulation was enough exploration for the scope of this class. I believe my code and output are mostly correct with appropriate changes that go outside of the class. When thinking about Jacksonian Networks, I wasnâ€™t expecting to use the transition matrices when changing from queue to queue. The discussion are correct because of the way that Is et them up. And while my code and premise do not align with what it sates on LibreText and Wikipedia. I felt that when I was taking the entirety of STAT 545, I didnâ€™t understand the use of learning stochastic processes until I started reading about queuing theory. Especially when it comes to the work place and management and Jackson Networks. Itâ€™s obvious how management science uses this concept a lot and how they rely on graphs like the ones above. Not only could this work in queues, but call centers, product lines, DMV lines, and so much more. research into other areas I didnâ€™t think could be useful to apply towards."
  },
  {
    "objectID": "posts/STAT551Project/index.html",
    "href": "posts/STAT551Project/index.html",
    "title": "School Modality Project",
    "section": "",
    "text": "School Modality Project\nOur School Modality Project examines the factors that influenced school districtsâ€™ decisions to adopt in-person, hybrid, or remote learning during the 2020-2021 school year. Using publicly available data from the CDC and school finance databases, we analyzed variables such as state, per-student spending, student enrollment, poverty rate, and demographics. Our findings indicate that districts with a higher proportion of white students were more likely to remain in-person, while larger districts and those with higher per-student spending tended to adopt hybrid or remote learning. Funding played a significant role in these decisions, as predominantly white districts received more resources, and higher spending facilitated smoother transitions to online learning through better access to technology. While our analysis provides valuable insights, we acknowledge ethical considerations such as missing IEP data, unclear terminology, and the need for equitable resource distribution. Ultimately, our project highlights how financial disparities and district size shaped learning modality decisions, emphasizing the importance of funding in determining educational access during the pandemic."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "SAT Statewide Categorical Analysis Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nAdam Kong, Sabrina Ahrendt, Franchesca Garcia, Julius Hoffman, Alex Tran\n\n\n\n\n\n\n  \n\n\n\n\nMultiplayer Server Queue Simulation\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSchool Modality Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\nAdam Kong, Alisa Krasilnikov, Dylan Le\n\n\n\n\n\n\n  \n\n\n\n\nBrawl Stars Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSushi DMAIC Process Improvement\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nSUPERCELL Game Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\n2020 World Series Debate\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\n2023 Basketball Bayesian Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nAdam Kong, Cameron An\n\n\n\n\n\n\n  \n\n\n\n\nCirrhosis Survival Analysis\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nAdam Kong\n\n\n\n\n\n\n  \n\n\n\n\nUS Honey Production Project\n\n\n\n\n\n\n\nProject\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAdam Kong, Alisa Krasilnikov, Harshini Karthikeyan\n\n\n\n\n\n\nNo matching items"
  }
]